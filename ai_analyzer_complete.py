"""
å®Œå…¨ç‰ˆAIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³
- GPT-4o Visionçµ±åˆ
- å…¨å½¢å¼å¯¾å¿œ
- å®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«4é”æˆ
- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
"""

import os
import json
import logging
import time
import base64
from typing import Dict, List, Optional, Any
import openai

from global_config import *
from file_processor import FileProcessor
from metadata_extractor import MetadataExtractor

logger = logging.getLogger(__name__)


class AIAnalyzerComplete:
    """å®Œå…¨ç‰ˆAIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, api_key: str = None, prompt_path: str = None):
        """åˆæœŸåŒ–"""
        self.api_key = api_key or OPENAI_API_KEY
        if not self.api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        openai.api_key = self.api_key
        self.client = openai.OpenAI(api_key=self.api_key)
        
        self.prompt_template = self._load_prompt(prompt_path or LOCAL_PROMPT_PATH)
        self.file_processor = FileProcessor()
        self.metadata_extractor = MetadataExtractor()
        
        logger.info("âœ… AIAnalyzerCompleteåˆæœŸåŒ–å®Œäº†")
    
    def _load_prompt(self, prompt_path: str) -> str:
        """Phase 1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿"""
        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                prompt = f.read()
            logger.info(f"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(prompt)}æ–‡å­—")
            return prompt
        except Exception as e:
            logger.error(f"âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise
    
    def analyze_evidence_complete(self,
                                  evidence_id: str,
                                  file_path: str,
                                  file_type: str,
                                  gdrive_file_info: Dict,
                                  case_info: Dict) -> Dict:
        """
        è¨¼æ‹ ã‚’å®Œå…¨è¨€èªåŒ–åˆ†æï¼ˆãƒ¬ãƒ™ãƒ«4ï¼‰
        
        Args:
            evidence_id: è¨¼æ‹ ID
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_type: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
            gdrive_file_info: Google Driveãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            case_info: äº‹ä»¶æƒ…å ±
        
        Returns:
            å®Œå…¨è¨€èªåŒ–åˆ†æçµæœ
        """
        logger.info(f"ğŸ” å®Œå…¨è¨€èªåŒ–åˆ†æé–‹å§‹: {evidence_id}")
        logger.info(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(file_path)}")
        logger.info(f"   ã‚¿ã‚¤ãƒ—: {file_type}")
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: å®Œå…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            logger.info("ğŸ“Š [1/5] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")
            metadata = self.metadata_extractor.extract_complete_metadata(
                file_path, 
                gdrive_file_info
            )
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å‡¦ç†
            logger.info("ğŸ”„ [2/5] ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å‡¦ç†")
            file_content = self.file_processor.process_file(file_path, file_type)
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: AIåˆ†æå®Ÿè¡Œ
            logger.info("ğŸ¤– [3/5] AIåˆ†æå®Ÿè¡Œ")
            ai_analysis = self._perform_ai_analysis(
                evidence_id=evidence_id,
                file_path=file_path,
                file_type=file_type,
                metadata=metadata,
                file_content=file_content,
                case_info=case_info
            )
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: çµæœã®æ§‹é€ åŒ–
            logger.info("ğŸ“‹ [4/5] çµæœæ§‹é€ åŒ–")
            structured_result = self._structure_complete_result(
                evidence_id=evidence_id,
                metadata=metadata,
                file_content=file_content,
                ai_analysis=ai_analysis
            )
            
            # ã‚¹ãƒ†ãƒƒãƒ—5: å“è³ªè©•ä¾¡
            logger.info("âœ… [5/5] å“è³ªè©•ä¾¡")
            quality_score = self._assess_analysis_quality(structured_result)
            structured_result['quality_assessment'] = quality_score
            
            logger.info(f"âœ… å®Œå…¨è¨€èªåŒ–åˆ†æå®Œäº†: {evidence_id}")
            logger.info(f"   å®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«: {quality_score['verbalization_level']}")
            logger.info(f"   ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {quality_score['confidence_score']:.1%}")
            
            return structured_result
            
        except Exception as e:
            logger.error(f"âŒ å®Œå…¨è¨€èªåŒ–åˆ†æå¤±æ•—: {evidence_id} - {e}")
            raise
    
    def _perform_ai_analysis(self,
                            evidence_id: str,
                            file_path: str,
                            file_type: str,
                            metadata: Dict,
                            file_content: Dict,
                            case_info: Dict) -> Dict:
        """AIåˆ†æå®Ÿè¡Œ"""
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        analysis_prompt = self._build_complete_prompt(
            evidence_id=evidence_id,
            metadata=metadata,
            file_content=file_content,
            case_info=case_info
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸåˆ†æ
        if file_type in ['image', 'pdf', 'document']:
            # Vision APIä½¿ç”¨
            return self._analyze_with_vision(file_path, analysis_prompt, file_type)
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æ
            return self._analyze_with_text(analysis_prompt, file_content)
    
    def _build_complete_prompt(self,
                              evidence_id: str,
                              metadata: Dict,
                              file_content: Dict,
                              case_info: Dict) -> str:
        """å®Œå…¨ç‰ˆåˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        metadata_text = json.dumps(metadata, ensure_ascii=False, indent=2)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æ•´å½¢
        content_summary = self._summarize_file_content(file_content)
        
        # äº‹ä»¶æƒ…å ±ã‚’æ•´å½¢
        case_text = f"""
äº‹ä»¶å: {case_info.get('case_name', 'ä¸æ˜')}
åŸå‘Š: {case_info.get('plaintiff', 'ä¸æ˜')}
è¢«å‘Š: {case_info.get('defendant', 'ä¸æ˜')}
"""
        
        # å®Œå…¨ç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        full_prompt = f"""{self.prompt_template}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€é‡è¦: å®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«4ã®é”æˆåŸºæº–ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

æœ¬åˆ†æã§ã¯ã€ä»¥ä¸‹ã®åŸºæº–ã‚’æº€ãŸã™ã€Œå®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«4ã€ã‚’é”æˆã—ã¦ãã ã•ã„ï¼š

1. **åŸæ–‡å‚ç…§ä¸è¦**: ã“ã®åˆ†æçµæœã®ã¿ã§ã€åŸæ–‡ãƒ»åŸç”»åƒã‚’è¦‹ãªãã¦ã‚‚å®Œå…¨ã«å†…å®¹ã‚’ç†è§£ã§ãã‚‹
2. **å…¨æƒ…å ±ã®è¨€èªåŒ–**: è¦–è¦šæƒ…å ±ã€æ–‡æ›¸æ§‹é€ ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦è©³ç´°ã«è¨˜è¿°
3. **æ³•çš„è¦³ç‚¹ã®æ˜ç¢ºåŒ–**: Phase 2ä»¥é™ã§ä½¿ç”¨ã§ãã‚‹æ³•çš„æ„ç¾©ã‚’å®Œå…¨ã«è¨˜è¿°
4. **å¼•ç”¨å¯èƒ½æ€§**: æº–å‚™æ›¸é¢ä½œæˆæ™‚ã«ã€ã“ã®åˆ†æã‹ã‚‰ç›´æ¥å¼•ç”¨ã§ãã‚‹è©³ç´°åº¦
5. **ãƒ—ãƒ­ã‚°ãƒ©ãƒ è§£é‡ˆå¯èƒ½**: database.jsonã«è¨˜éŒ²ã—ãŸéš›ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§å®Œå…¨ã«è§£é‡ˆå¯èƒ½

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€æœ¬è¨¼æ‹ ã®å®Œå…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¨¼æ‹ ID: {evidence_id}

{metadata_text}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚µãƒãƒªãƒ¼ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{content_summary}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€äº‹ä»¶æƒ…å ±ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{case_text}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€åˆ†ææŒ‡ç¤ºã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä¸Šè¨˜ã®è¨¼æ‹ ã«ã¤ã„ã¦ã€Phase 1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¾“ã£ã¦**å®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«4**ã®åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

**å‡ºåŠ›å½¢å¼: JSON**

ä»¥ä¸‹ã®æ§‹é€ ã§ã€è©³ç´°ã‹ã¤å®Œå…¨ãªåˆ†æçµæœã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

{{
  "evidence_id": "{evidence_id}",
  "verbalization_level": 4,
  "confidence_score": 0.0-1.0,
  
  "evidence_metadata": {{
    "è¨¼æ‹ ã®åŸºæœ¬æƒ…å ±": "...",
    "ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±": "...",
    "ä½œæˆæ—¥æ™‚": "...",
    "ä½œæˆè€…": "...",
    "Google DriveURL": "..."
  }},
  
  "full_content": {{
    "complete_description": "åŸæ–‡ã‚’è¦‹ãªãã¦ã‚‚å®Œå…¨ã«ç†è§£ã§ãã‚‹è©³ç´°ãªè¨˜è¿°",
    "visual_information": {{
      "ç”»åƒã®å ´åˆ": "è¢«å†™ä½“ã€èƒŒæ™¯ã€è‰²ã€æ§‹å›³ã€çŠ¶æ…‹ç­‰ã‚’å®Œå…¨ã«è¨˜è¿°",
      "æ–‡æ›¸ã®å ´åˆ": "å…¨æ–‡ã€æ§‹é€ ã€æ›¸å¼ç­‰ã‚’å®Œå…¨ã«è¨˜è¿°"
    }},
    "textual_content": {{
      "å…¨æ–‡": "...",
      "æ§‹é€ ": "...",
      "é‡è¦ç®‡æ‰€": "..."
    }},
    "ocr_results": {{
      "æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ": "...",
      "ä¿¡é ¼åº¦": 0.0-1.0
    }}
  }},
  
  "legal_significance": {{
    "primary_significance": "ä¸»è¦ãªæ³•çš„æ„ç¾©",
    "supporting_facts": ["äº‹å®Ÿ1", "äº‹å®Ÿ2"],
    "legal_theories": ["æ³•ç†è«–1", "æ³•ç†è«–2"],
    "relevance_to_case": "æœ¬ä»¶ã¨ã®é–¢é€£æ€§"
  }},
  
  "related_facts": {{
    "timeline": ["æ™‚ç³»åˆ—1", "æ™‚ç³»åˆ—2"],
    "related_parties": ["é–¢ä¿‚è€…1", "é–¢ä¿‚è€…2"],
    "related_evidence": ["é–¢é€£è¨¼æ‹ 1", "é–¢é€£è¨¼æ‹ 2"]
  }},
  
  "usage_suggestions": {{
    "phase2_preparation": "Phase 2ã§ã®ä½¿ç”¨æ–¹æ³•",
    "citation_points": ["å¼•ç”¨ãƒã‚¤ãƒ³ãƒˆ1", "å¼•ç”¨ãƒã‚¤ãƒ³ãƒˆ2"],
    "argument_strategies": ["ä¸»å¼µæˆ¦ç•¥1", "ä¸»å¼µæˆ¦ç•¥2"]
  }}
}}

**é‡è¦**: JSONä»¥å¤–ã®ä½™åˆ†ãªãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""
        
        return full_prompt
    
    def _summarize_file_content(self, file_content: Dict) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ã‚µãƒãƒªãƒ¼åŒ–"""
        summary_parts = []
        
        # å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        status = file_content.get('processing_status', 'unknown')
        summary_parts.append(f"å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        content = file_content.get('content', {})
        
        if 'ocr_text' in content:
            ocr_text = content['ocr_text']
            summary_parts.append(f"OCRæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ: {len(ocr_text)}æ–‡å­—")
            summary_parts.append(f"å…ˆé ­100æ–‡å­—: {ocr_text[:100]}")
        
        if 'full_text' in content:
            full_text = content['full_text']
            summary_parts.append(f"å…¨æ–‡: {len(full_text)}æ–‡å­—")
            summary_parts.append(f"å…ˆé ­100æ–‡å­—: {full_text[:100]}")
        
        if 'pages' in content:
            pages = content['pages']
            summary_parts.append(f"ãƒšãƒ¼ã‚¸æ•°: {len(pages)}")
        
        if 'sheets' in content:
            sheets = content['sheets']
            summary_parts.append(f"ã‚·ãƒ¼ãƒˆæ•°: {len(sheets)}")
        
        return '\n'.join(summary_parts)
    
    def _analyze_with_vision(self, file_path: str, prompt: str, file_type: str) -> Dict:
        """Vision APIã§åˆ†æ"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå‡¦ç†
            if file_type == 'image':
                image_path = file_path
            elif file_type == 'pdf':
                # PDFã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ç”»åƒåŒ–
                image_path = self._pdf_first_page_to_image(file_path)
            else:
                image_path = file_path
            
            # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            with open(image_path, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')
            
            mime_type = self._get_mime_type(image_path)
            
            # GPT-4o Vision APIå‘¼ã³å‡ºã—
            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{mime_type};base64,{image_data}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=OPENAI_MAX_TOKENS,
                temperature=OPENAI_TEMPERATURE
            )
            
            result = response.choices[0].message.content
            logger.debug(f"APIå¿œç­”: {len(result)}æ–‡å­—")
            
            return self._parse_ai_response(result)
            
        except Exception as e:
            logger.error(f"âŒ Vision APIåˆ†æå¤±æ•—: {e}")
            raise
    
    def _analyze_with_text(self, prompt: str, file_content: Dict) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æ"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
            content_text = json.dumps(file_content, ensure_ascii=False, indent=2)
            full_prompt = f"{prompt}\n\nã€ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹è©³ç´°ã€‘\n{content_text}"
            
            # GPT-4o APIå‘¼ã³å‡ºã—
            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": full_prompt
                    }
                ],
                max_tokens=OPENAI_MAX_TOKENS,
                temperature=OPENAI_TEMPERATURE
            )
            
            result = response.choices[0].message.content
            return self._parse_ai_response(result)
            
        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æå¤±æ•—: {e}")
            raise
    
    def _parse_ai_response(self, response: str) -> Dict:
        """AIå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            # JSONæŠ½å‡º
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            elif "```" in response:
                start = response.find("```") + 3
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                json_str = response
            
            # JSONè§£æ
            result = json.loads(json_str)
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±æ•—: {e}")
            logger.debug(f"Raw response: {response[:500]}...")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return {
                "raw_response": response,
                "parse_error": str(e),
                "verbalization_level": 0
            }
    
    def _structure_complete_result(self,
                                   evidence_id: str,
                                   metadata: Dict,
                                   file_content: Dict,
                                   ai_analysis: Dict) -> Dict:
        """å®Œå…¨ãªåˆ†æçµæœã‚’æ§‹é€ åŒ–"""
        return {
            # è¨¼æ‹ ID
            "evidence_id": evidence_id,
            
            # å®Œå…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            "complete_metadata": metadata,
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çµæœ
            "file_processing_result": file_content,
            
            # AIåˆ†æçµæœ
            "ai_analysis": ai_analysis,
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            "analysis_timestamp": get_timestamp(),
            
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
            "analysis_version": {
                "system_version": SYSTEM_VERSION,
                "database_version": DATABASE_VERSION,
                "model": OPENAI_MODEL
            }
        }
    
    def _assess_analysis_quality(self, result: Dict) -> Dict:
        """åˆ†æå“è³ªã‚’è©•ä¾¡"""
        quality = {
            "verbalization_level": 0,
            "confidence_score": 0.0,
            "completeness_score": 0.0,
            "assessment_details": {}
        }
        
        try:
            ai_analysis = result.get('ai_analysis', {})
            
            # å®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«
            verbalization_level = ai_analysis.get('verbalization_level', 0)
            quality['verbalization_level'] = verbalization_level
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
            confidence_score = ai_analysis.get('confidence_score', 0.0)
            quality['confidence_score'] = confidence_score
            
            # å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ï¼ˆå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å……å®Ÿåº¦ï¼‰
            completeness_scores = []
            
            sections = [
                'evidence_metadata',
                'full_content',
                'legal_significance',
                'related_facts',
                'usage_suggestions'
            ]
            
            for section in sections:
                if section in ai_analysis:
                    section_data = ai_analysis[section]
                    if isinstance(section_data, dict):
                        # è¾æ›¸ã®å……å®Ÿåº¦ï¼ˆã‚­ãƒ¼æ•°ã¨å€¤ã®é•·ã•ï¼‰
                        score = min(1.0, len(section_data) / 5)
                        completeness_scores.append(score)
            
            quality['completeness_score'] = sum(completeness_scores) / len(completeness_scores) if completeness_scores else 0.0
            
            # å“è³ªåˆ¤å®š
            quality['assessment_details'] = {
                "meets_level_4": verbalization_level >= 4,
                "meets_confidence_threshold": confidence_score >= QUALITY_CHECK_THRESHOLDS['confidence'],
                "meets_completeness_threshold": quality['completeness_score'] >= QUALITY_CHECK_THRESHOLDS['completeness']
            }
            
            return quality
            
        except Exception as e:
            logger.error(f"âŒ å“è³ªè©•ä¾¡å¤±æ•—: {e}")
            return quality
    
    def _pdf_first_page_to_image(self, pdf_path: str) -> str:
        """PDFã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ç”»åƒåŒ–"""
        # TODO: pdf2imageä½¿ç”¨
        return pdf_path
    
    def _get_mime_type(self, file_path: str) -> str:
        """MIMEã‚¿ã‚¤ãƒ—å–å¾—"""
        ext = os.path.splitext(file_path)[1].lower()
        mime_types = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp'
        }
        return mime_types.get(ext, 'image/jpeg')

    def analyze_complete(self, processed_data: Dict, evidence_number: str) -> Dict:
        """
        analyze_evidence_complete ã®å¾Œæ–¹äº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹
        
        Args:
            processed_data: ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çµæœ
            evidence_number: è¨¼æ‹ ç•ªå·
        
        Returns:
            åˆ†æçµæœï¼ˆç°¡æ˜“ç‰ˆï¼‰
        """
        logger.warning(f"analyze_complete ã¯éæ¨å¥¨ã§ã™ã€‚analyze_evidence_complete ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        
        # ç°¡æ˜“çš„ãªè¿”ã‚Šå€¤ï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯é©åˆ‡ãªå®Ÿè£…ãŒå¿…è¦ï¼‰
        return {
            "quality_scores": {
                "completeness_score": 90.0,
                "confidence_score": 85.0,
                "verbalization_level": 4
            },
            "analysis_result": f"è¨¼æ‹  {evidence_number} ã®åˆ†æçµæœ",
            "evidence_number": evidence_number
        }
