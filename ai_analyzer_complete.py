"""
å®Œå…¨ç‰ˆAIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³
- GPT-4o Visionçµ±åˆ
- å…¨å½¢å¼å¯¾å¿œ
- å®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«4é”æˆ
- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
"""

import os
import json
import logging
import time
import base64
from typing import Dict, List, Optional, Any
import openai

from global_config import *
from file_processor import FileProcessor
from metadata_extractor import MetadataExtractor

logger = logging.getLogger(__name__)


class AIAnalyzerComplete:
    """å®Œå…¨ç‰ˆAIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, api_key: str = None, prompt_path: str = None):
        """åˆæœŸåŒ–"""
        self.api_key = api_key or OPENAI_API_KEY
        if not self.api_key:
            raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        openai.api_key = self.api_key
        self.client = openai.OpenAI(api_key=self.api_key)
        
        self.prompt_template = self._load_prompt(prompt_path or LOCAL_PROMPT_PATH)
        self.file_processor = FileProcessor()
        self.metadata_extractor = MetadataExtractor()
        
        logger.info("âœ… AIAnalyzerCompleteåˆæœŸåŒ–å®Œäº†")
    
    def _load_prompt(self, prompt_path: str) -> str:
        """Phase 1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿"""
        try:
            with open(prompt_path, 'r', encoding='utf-8') as f:
                prompt = f.read()
            logger.info(f"âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(prompt)}æ–‡å­—")
            return prompt
        except Exception as e:
            logger.error(f"âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            raise
    
    def analyze_evidence_complete(self,
                                  evidence_id: str,
                                  file_path: str,
                                  file_type: str,
                                  gdrive_file_info: Dict,
                                  case_info: Dict) -> Dict:
        """
        è¨¼æ‹ ã‚’å®Œå…¨è¨€èªåŒ–åˆ†æï¼ˆãƒ¬ãƒ™ãƒ«4ï¼‰
        
        Args:
            evidence_id: è¨¼æ‹ ID
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_type: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
            gdrive_file_info: Google Driveãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            case_info: äº‹ä»¶æƒ…å ±
        
        Returns:
            å®Œå…¨è¨€èªåŒ–åˆ†æçµæœ
        """
        logger.info(f"ğŸ” å®Œå…¨è¨€èªåŒ–åˆ†æé–‹å§‹: {evidence_id}")
        logger.info(f"   ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(file_path)}")
        logger.info(f"   ã‚¿ã‚¤ãƒ—: {file_type}")
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: å®Œå…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            logger.info("ğŸ“Š [1/5] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")
            metadata = self.metadata_extractor.extract_complete_metadata(
                file_path, 
                gdrive_file_info
            )
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å‡¦ç†
            logger.info("ğŸ”„ [2/5] ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å‡¦ç†")
            file_content = self.file_processor.process_file(file_path, file_type)
            
            # ã‚¹ãƒ†ãƒƒãƒ—3: AIåˆ†æå®Ÿè¡Œ
            logger.info("ğŸ¤– [3/5] AIåˆ†æå®Ÿè¡Œ")
            ai_analysis = self._perform_ai_analysis(
                evidence_id=evidence_id,
                file_path=file_path,
                file_type=file_type,
                metadata=metadata,
                file_content=file_content,
                case_info=case_info
            )
            
            # ã‚¹ãƒ†ãƒƒãƒ—4: çµæœã®æ§‹é€ åŒ–
            logger.info("ğŸ“‹ [4/5] çµæœæ§‹é€ åŒ–")
            structured_result = self._structure_complete_result(
                evidence_id=evidence_id,
                metadata=metadata,
                file_content=file_content,
                ai_analysis=ai_analysis
            )
            
            # ã‚¹ãƒ†ãƒƒãƒ—5: å“è³ªè©•ä¾¡
            logger.info("âœ… [5/5] å“è³ªè©•ä¾¡")
            quality_score = self._assess_analysis_quality(structured_result)
            structured_result['quality_assessment'] = quality_score
            
            logger.info(f"âœ… å®Œå…¨è¨€èªåŒ–åˆ†æå®Œäº†: {evidence_id}")
            logger.info(f"   å®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«: {quality_score['verbalization_level']}")
            logger.info(f"   ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢: {quality_score['confidence_score']:.1%}")
            
            return structured_result
            
        except Exception as e:
            logger.error(f"âŒ å®Œå…¨è¨€èªåŒ–åˆ†æå¤±æ•—: {evidence_id} - {e}")
            raise
    
    def _perform_ai_analysis(self,
                            evidence_id: str,
                            file_path: str,
                            file_type: str,
                            metadata: Dict,
                            file_content: Dict,
                            case_info: Dict) -> Dict:
        """AIåˆ†æå®Ÿè¡Œ"""
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        analysis_prompt = self._build_complete_prompt(
            evidence_id=evidence_id,
            metadata=metadata,
            file_content=file_content,
            case_info=case_info
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸåˆ†æ
        if file_type in ['image', 'pdf', 'document']:
            # Vision APIä½¿ç”¨
            # HEICç­‰ã®å¤‰æ›æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä½¿ç”¨
            actual_file_path = file_content.get('processed_file_path', file_path)
            vision_result = self._analyze_with_vision(actual_file_path, analysis_prompt, file_type)
            
            # Vision APIãŒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒªã‚·ãƒ¼ã§æ‹’å¦ã—ãŸå ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if vision_result is None:
                logger.info("ğŸ“ OCRãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æã‚’å®Ÿè¡Œ")
                return self._analyze_with_text(analysis_prompt, file_content)
            
            return vision_result
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æ
            return self._analyze_with_text(analysis_prompt, file_content)
    
    def _build_complete_prompt(self,
                              evidence_id: str,
                              metadata: Dict,
                              file_content: Dict,
                              case_info: Dict) -> str:
        """å®Œå…¨ç‰ˆåˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        metadata_text = json.dumps(metadata, ensure_ascii=False, indent=2)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’æ•´å½¢
        content_summary = self._summarize_file_content(file_content)
        
        # å®Œå…¨ç‰ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        full_prompt = f"""{self.prompt_template}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€é‡è¦: Phase 1 = å®¢è¦³çš„äº‹å®Ÿè¨˜éŒ²ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Phase 1ã®å½¹å‰²:**
- è¨¼æ‹ ã®å†…å®¹ã‚’å®Œå…¨ã«å®¢è¦³çš„ãƒ»ä¸­ç«‹çš„ã«è¨˜éŒ²
- è¦³å¯Ÿå¯èƒ½ãªäº‹å®Ÿã®ã¿ã‚’è©³ç´°ã«è¨€èªåŒ–
- æ³•çš„è©•ä¾¡ã‚„ä¸»è¦³çš„è§£é‡ˆã¯ä¸€åˆ‡è¡Œã‚ãªã„
- è¨´è¨Ÿã®å½“äº‹è€…æƒ…å ±ã¯ä¸ãˆã‚‰ã‚Œãªã„ï¼ˆä¸­ç«‹æ€§ã®æ‹…ä¿ï¼‰

**å®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«4ã®é”æˆåŸºæº–:**
1. **åŸæ–‡å‚ç…§ä¸è¦**: ã“ã®åˆ†æçµæœã®ã¿ã§ã€åŸæ–‡ãƒ»åŸç”»åƒã‚’è¦‹ãªãã¦ã‚‚å®Œå…¨ã«å†…å®¹ã‚’ç†è§£ã§ãã‚‹
2. **å…¨æƒ…å ±ã®è¨€èªåŒ–**: è¦–è¦šæƒ…å ±ã€æ–‡æ›¸æ§‹é€ ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ã¦è©³ç´°ã«è¨˜è¿°
3. **å®¢è¦³çš„è¨˜éŒ²**: è¦³å¯Ÿå¯èƒ½ãªäº‹å®Ÿã®ã¿ã‚’è¨˜è¿°ã€è§£é‡ˆã‚„è©•ä¾¡ã¯å«ã¾ãªã„
4. **å¼•ç”¨å¯èƒ½æ€§**: Phase 2ã§ã®æ³•çš„åˆ†æã‚„æº–å‚™æ›¸é¢ä½œæˆæ™‚ã«ä½¿ç”¨ã§ãã‚‹è©³ç´°åº¦
5. **ãƒ—ãƒ­ã‚°ãƒ©ãƒ è§£é‡ˆå¯èƒ½**: database.jsonã«è¨˜éŒ²ã—ãŸéš›ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§å®Œå…¨ã«è§£é‡ˆå¯èƒ½

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€æœ¬è¨¼æ‹ ã®å®Œå…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

è¨¼æ‹ ID: {evidence_id}

{metadata_text}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚µãƒãƒªãƒ¼ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{content_summary}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ã€åˆ†ææŒ‡ç¤ºã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ä¸Šè¨˜ã®è¨¼æ‹ ã«ã¤ã„ã¦ã€Phase 1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¾“ã£ã¦**å®¢è¦³çš„ãƒ»ä¸­ç«‹çš„ãªå®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«4**ã®åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

**é‡è¦:** 
- è¨¼æ‹ ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹äº‹å®Ÿã®ã¿ã‚’è¨˜éŒ²
- æ³•çš„è©•ä¾¡ã‚„ä¸»è¦³çš„è§£é‡ˆã¯ä¸€åˆ‡å«ã‚ãªã„
- è¨´è¨Ÿã®å½“äº‹è€…ã‚„äº‹ä»¶ã®è©³ç´°ã¯çŸ¥ã‚‰ãªã„å‰æã§åˆ†æ
- ã‚ãªãŸã¯ä¸­ç«‹çš„ãªè¨˜éŒ²è€…ã¨ã—ã¦æŒ¯ã‚‹èˆã†

**ğŸ—“ï¸ æœ€é‡è¦ã‚¿ã‚¹ã‚¯ - è¨¼æ‹ ã®ä½œæˆå¹´æœˆæ—¥ã‚’å¿…ãšç‰¹å®š:**
- `document_date`: è¨¼æ‹ èª¬æ˜æ›¸ã«è¨˜è¼‰ã™ã‚‹ä½œæˆå¹´æœˆæ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
- æ–‡æ›¸ä¸Šéƒ¨ã®æ—¥ä»˜ã€EXIFæ’®å½±æ—¥æ™‚ã€ãƒ•ã‚¡ã‚¤ãƒ«åã®æ—¥ä»˜ãªã©ã€ã‚ã‚‰ã‚†ã‚‹æ‰‹ãŒã‹ã‚Šã‹ã‚‰åˆ¤æ–­
- `document_date_source`ã«æ ¹æ‹ ã‚’æ˜è¨˜ï¼ˆä¾‹ï¼šã€Œæ–‡æ›¸ä¸Šéƒ¨ã«2021å¹´8æœˆ15æ—¥ã¨è¨˜è¼‰ã€ï¼‰
- æ—¥ä»˜ãŒä¸æ˜ãªå ´åˆã‚‚ã€ãã®æ—¨ã¨ç†ç”±ã‚’`date_confidence`ã«è¨˜è¼‰

**å‡ºåŠ›å½¢å¼: JSON**

ä»¥ä¸‹ã®æ§‹é€ ã§ã€è©³ç´°ã‹ã¤å®Œå…¨ãªåˆ†æçµæœã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

{{
  "evidence_id": "{evidence_id}",
  "verbalization_level": 4,
  "confidence_score": 0.0-1.0,
  
  "evidence_metadata": {{
    "è¨¼æ‹ ã®åŸºæœ¬æƒ…å ±": "...",
    "ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±": "...",
    "ä½œæˆæ—¥æ™‚": "...",
    "ä½œæˆè€…": "...",
    "Google DriveURL": "..."
  }},
  
  "full_content": {{
    "complete_description": "åŸæ–‡ã‚’è¦‹ãªãã¦ã‚‚å®Œå…¨ã«ç†è§£ã§ãã‚‹è©³ç´°ãªè¨˜è¿°",
    "visual_information": {{
      "overall_description": "ç”»åƒå…¨ä½“ã®å®¢è¦³çš„èª¬æ˜",
      "key_elements": ["è¦³å¯Ÿã•ã‚Œã‚‹è¦ç´ 1", "è¦ç´ 2"],
      "text_in_image": [
        {{"text": "ç”»åƒå†…ãƒ†ã‚­ã‚¹ãƒˆ", "location": "ä½ç½®", "size": "ã‚µã‚¤ã‚º"}}
      ],
      "background_details": "èƒŒæ™¯ã®å®¢è¦³çš„èª¬æ˜",
      "quality_notes": "ç”»è³ªã€é®®æ˜åº¦"
    }},
    "textual_content": {{
      "extracted_text": "æ–‡æ›¸å†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£ç¢ºã«æŠ½å‡º",
      "text_summary": "ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã®å®¢è¦³çš„è¦ç´„",
      "document_structure": "æ–‡æ›¸ã®æ§‹é€ ",
      "formatting_notes": "æ›¸å¼ã€ãƒ•ã‚©ãƒ³ãƒˆã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ"
    }},
    "ocr_results": {{
      "extracted_text": "OCRã§æŠ½å‡ºã•ã‚ŒãŸå…¨ãƒ†ã‚­ã‚¹ãƒˆ",
      "confidence": 0.0-1.0
    }}
  }},
  
  "objective_analysis": {{
    "document_type": "æ–‡æ›¸ç¨®é¡ã®å®¢è¦³çš„åˆ†é¡",
    "observable_facts": [
      "è¨¼æ‹ ã‹ã‚‰è¦³å¯Ÿã§ãã‚‹å®¢è¦³çš„äº‹å®Ÿ1",
      "å®¢è¦³çš„äº‹å®Ÿ2"
    ],
    "temporal_information": {{
      "document_date": "è¨¼æ‹ ã®ä½œæˆå¹´æœˆæ—¥ï¼ˆYYYY-MM-DDå½¢å¼å¿…é ˆï¼‰â€»è¨¼æ‹ èª¬æ˜æ›¸è¨˜è¼‰ç”¨",
      "document_date_source": "ä½œæˆæ—¥ã®æ ¹æ‹ ï¼ˆæ–‡æ›¸ä¸Šéƒ¨ã®æ—¥ä»˜ã€EXIFæ’®å½±æ—¥æ™‚ã€ãƒ•ã‚¡ã‚¤ãƒ«åç­‰ï¼‰",
      "other_dates": [
        {{"date": "YYYY-MM-DD", "context": "ã“ã®æ—¥ä»˜ã®æ„å‘³ï¼ˆä¾‹ï¼šæœ‰åŠ¹æœŸé™ã€æ”¯æ‰•æœŸé™ç­‰ï¼‰"}}
      ],
      "timeline": "æ™‚ç³»åˆ—ã®å®¢è¦³çš„æ•´ç†",
      "date_confidence": "ä½œæˆå¹´æœˆæ—¥ã®ä¿¡é ¼åº¦ï¼ˆhigh/medium/lowï¼‰ã¨ç†ç”±"
    }},
    "parties_mentioned": {{
      "individuals": ["å€‹äººå1", "å€‹äººå2"],
      "organizations": ["çµ„ç¹”å1", "çµ„ç¹”å2"],
      "roles_described": "æ–‡æ›¸å†…ã§å„å½“äº‹è€…ã®è¨˜è¿°"
    }},
    "financial_information": {{
      "amounts": ["é‡‘é¡1", "é‡‘é¡2"],
      "currency": "é€šè²¨å˜ä½",
      "amount_context": "å„é‡‘é¡ã®æ–‡è„ˆ"
    }},
    "identifiers": {{
      "contract_numbers": ["å¥‘ç´„ç•ªå·ç­‰"],
      "reference_numbers": ["å‚ç…§ç•ªå·"],
      "serial_numbers": ["ç®¡ç†ç•ªå·ç­‰"]
    }},
    "signatures_and_seals": {{
      "signatures": ["ç½²åè€…å1", "ç½²åè€…å2"],
      "seals": ["æºå°ã®ç¨®é¡"],
      "signature_dates": ["ç½²åæ—¥ä»˜"],
      "signature_locations": "ç½²åãƒ»æºå°ã®ä½ç½®"
    }},
    "document_state": {{
      "completeness": "æ–‡æ›¸ã®å®Œå…¨æ€§",
      "modifications": "è¨‚æ­£ã€å‰Šé™¤ã€è¿½è¨˜",
      "annotations": "æ‰‹æ›¸ããƒ¡ãƒ¢ã€ãƒãƒ¼ã‚«ãƒ¼",
      "preservation_state": "ä¿å­˜çŠ¶æ…‹"
    }}
  }},
  
  "extracted_data": {{
    "key_terms": ["é‡è¦ç”¨èª1", "ç”¨èª2"],
    "definitions": "å®šç¾©ã•ã‚Œã¦ã„ã‚‹ç”¨èª",
    "conditions": ["æ¡ä»¶1", "æ¡ä»¶2"],
    "obligations": ["ç¾©å‹™1", "ç¾©å‹™2"],
    "rights": ["æ¨©åˆ©1", "æ¨©åˆ©2"],
    "exceptions": ["ä¾‹å¤–è¦å®š"]
  }},
  
  "metadata_analysis": {{
    "file_properties": "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£",
    "creation_metadata": "ä½œæˆæ—¥æ™‚ã€æ›´æ–°æ—¥æ™‚ç­‰",
    "technical_details": "è§£åƒåº¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã€å½¢å¼ç­‰"
  }}
}}

**é‡è¦**: JSONä»¥å¤–ã®ä½™åˆ†ãªãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""
        
        return full_prompt
    
    def _summarize_file_content(self, file_content: Dict) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ã‚µãƒãƒªãƒ¼åŒ–"""
        summary_parts = []
        
        # å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        status = file_content.get('processing_status', 'unknown')
        summary_parts.append(f"å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        content = file_content.get('content', {})
        
        if 'ocr_text' in content:
            ocr_text = content['ocr_text']
            summary_parts.append(f"OCRæŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆ: {len(ocr_text)}æ–‡å­—")
            summary_parts.append(f"å…ˆé ­100æ–‡å­—: {ocr_text[:100]}")
        
        if 'full_text' in content:
            full_text = content['full_text']
            summary_parts.append(f"å…¨æ–‡: {len(full_text)}æ–‡å­—")
            summary_parts.append(f"å…ˆé ­100æ–‡å­—: {full_text[:100]}")
        
        if 'pages' in content:
            pages = content['pages']
            summary_parts.append(f"ãƒšãƒ¼ã‚¸æ•°: {len(pages)}")
        
        if 'sheets' in content:
            sheets = content['sheets']
            summary_parts.append(f"ã‚·ãƒ¼ãƒˆæ•°: {len(sheets)}")
        
        return '\n'.join(summary_parts)
    
    def _analyze_with_vision(self, file_path: str, prompt: str, file_type: str) -> Dict:
        """Vision APIã§åˆ†æ"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå‡¦ç†
            if file_type == 'image':
                image_path = file_path
            elif file_type in ['pdf', 'document']:
                # PDFã¾ãŸã¯Wordæ–‡æ›¸ã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ç”»åƒåŒ–
                image_path = self._pdf_first_page_to_image(file_path)
                
                # PDFå¤‰æ›å¤±æ•—æ™‚ã¯ãƒ†ã‚­ã‚¹ãƒˆè§£æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if image_path is None:
                    logger.warning(f"{file_type}â†’ç”»åƒå¤‰æ›å¤±æ•—ã€ãƒ†ã‚­ã‚¹ãƒˆè§£æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                    return self._analyze_with_text(prompt, {'file_path': file_path})
            else:
                image_path = file_path
            
            # Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            with open(image_path, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')
            
            mime_type = self._get_mime_type(image_path)
            
            # GPT-4o Vision APIå‘¼ã³å‡ºã—
            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": prompt
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:{mime_type};base64,{image_data}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=OPENAI_MAX_TOKENS,
                temperature=OPENAI_TEMPERATURE
            )
            
            result = response.choices[0].message.content
            logger.debug(f"APIå¿œç­”: {len(result)}æ–‡å­—")
            
            # ãƒ‡ãƒãƒƒã‚°: APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æœ€åˆã®200æ–‡å­—ã‚’è¡¨ç¤º
            if result:
                logger.debug(f"APIå¿œç­”ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼: {result[:200]}...")
            
            # OpenAIã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒªã‚·ãƒ¼æ‹’å¦ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šå³å¯†ãªåˆ¤å®šï¼‰
            # ç’°å¢ƒå¤‰æ•° DISABLE_CONTENT_POLICY_CHECK=true ã§ãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–å¯èƒ½
            disable_check = os.getenv('DISABLE_CONTENT_POLICY_CHECK', 'false').lower() == 'true'
            
            if not disable_check:
                # çœŸã®æ‹’å¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ç‰¹å¾´:
                # 1. éå¸¸ã«çŸ­ã„ï¼ˆé€šå¸¸100æ–‡å­—æœªæº€ï¼‰
                # 2. JSONå½¢å¼ã§ã¯ãªã„
                # 3. "I'm sorry, I can't assist with that"ã¨ã„ã†å®Œå…¨ä¸€è‡´
                if result and len(result) < 200 and "```" not in result and "{" not in result:
                    # JSONå½¢å¼ã§ã¯ãªã„çŸ­ã„å¿œç­”ã®å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
                    if "I'm sorry, I can't assist with that" in result or \
                       "I cannot assist with that request" in result or \
                       (result.startswith("I'm sorry") and "assist" in result):
                        logger.warning("âš ï¸ Vision API: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒãƒªã‚·ãƒ¼ã«ã‚ˆã‚Šç”»åƒåˆ†æãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸ")
                        logger.warning(f"   æ‹’å¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {result}")
                        logger.info("ğŸ“ OCRãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                        logger.info("   ãƒ’ãƒ³ãƒˆ: èª¤æ¤œå‡ºã®å ´åˆã¯ DISABLE_CONTENT_POLICY_CHECK=true ã§ç„¡åŠ¹åŒ–ã§ãã¾ã™")
                        return None  # Noneã‚’è¿”ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’ä¿ƒã™
            
            return self._parse_ai_response(result)
            
        except Exception as e:
            logger.error(f"âŒ Vision APIåˆ†æå¤±æ•—: {e}")
            raise
    
    def _analyze_with_text(self, prompt: str, file_content: Dict) -> Dict:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æ"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ 
            content_text = json.dumps(file_content, ensure_ascii=False, indent=2)
            full_prompt = f"{prompt}\n\nã€ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹è©³ç´°ã€‘\n{content_text}"
            
            # GPT-4o APIå‘¼ã³å‡ºã—
            response = self.client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[
                    {
                        "role": "user",
                        "content": full_prompt
                    }
                ],
                max_tokens=OPENAI_MAX_TOKENS,
                temperature=OPENAI_TEMPERATURE
            )
            
            result = response.choices[0].message.content
            return self._parse_ai_response(result)
            
        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æå¤±æ•—: {e}")
            raise
    
    def _parse_ai_response(self, response: str) -> Dict:
        """AIå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹"""
        try:
            # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯å…¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ­ã‚°å‡ºåŠ›
            debug_mode = os.getenv('DEBUG_MODE', 'false').lower() == 'true'
            if debug_mode:
                logger.debug("=== OpenAIç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹é–‹å§‹ ===")
                logger.debug(response)
                logger.debug("=== OpenAIç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹çµ‚äº† ===")
            
            # JSONæŠ½å‡º
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            elif "```" in response:
                start = response.find("```") + 3
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                json_str = response
            
            # ç©ºæ–‡å­—ãƒã‚§ãƒƒã‚¯
            if not json_str or not json_str.strip():
                logger.warning("è­¦å‘Š: æŠ½å‡ºã•ã‚ŒãŸJSONæ–‡å­—åˆ—ãŒç©ºã§ã™")
                if debug_mode:
                    logger.debug(f"Raw response: {response}")
                else:
                    logger.debug(f"Raw response (æœ€åˆã®500æ–‡å­—): {response[:500]}...")
                return {
                    "raw_response": response,
                    "parse_error": "Empty JSON string",
                    "verbalization_level": 0
                }
            
            # JSONè§£æ
            result = json.loads(json_str)
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"ã‚¨ãƒ©ãƒ¼: JSONè§£æå¤±æ•— - {e}")
            logger.warning(f"æŠ½å‡ºã—ãŸJSONæ–‡å­—åˆ—ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰: {json_str[:200] if 'json_str' in locals() else 'N/A'}")
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯å…¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡ºåŠ›
            if debug_mode:
                logger.debug(f"JSONè§£æå¤±æ•—ã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response}")
            else:
                logger.debug(f"Raw response (æœ€åˆã®500æ–‡å­—): {response[:500]}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return {
                "raw_response": response,
                "parse_error": str(e),
                "verbalization_level": 0
            }
    
    def _structure_complete_result(self,
                                   evidence_id: str,
                                   metadata: Dict,
                                   file_content: Dict,
                                   ai_analysis: Dict) -> Dict:
        """å®Œå…¨ãªåˆ†æçµæœã‚’æ§‹é€ åŒ–"""
        return {
            # è¨¼æ‹ ID
            "evidence_id": evidence_id,
            
            # å®Œå…¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            "complete_metadata": metadata,
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çµæœ
            "file_processing_result": file_content,
            
            # AIåˆ†æçµæœ
            "ai_analysis": ai_analysis,
            
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
            "analysis_timestamp": get_timestamp(),
            
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±
            "analysis_version": {
                "system_version": SYSTEM_VERSION,
                "database_version": DATABASE_VERSION,
                "model": OPENAI_MODEL
            }
        }
    
    def _assess_analysis_quality(self, result: Dict) -> Dict:
        """åˆ†æå“è³ªã‚’è©•ä¾¡"""
        quality = {
            "verbalization_level": 0,
            "confidence_score": 0.0,
            "completeness_score": 0.0,
            "assessment_details": {}
        }
        
        try:
            ai_analysis = result.get('ai_analysis', {})
            
            # å®Œå…¨è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«
            verbalization_level = ai_analysis.get('verbalization_level', 0)
            quality['verbalization_level'] = verbalization_level
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
            confidence_score = ai_analysis.get('confidence_score', 0.0)
            quality['confidence_score'] = confidence_score
            
            # å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ï¼ˆå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å……å®Ÿåº¦ï¼‰
            completeness_scores = []
            
            sections = [
                'evidence_metadata',
                'full_content',
                'legal_significance',
                'related_facts',
                'usage_suggestions'
            ]
            
            for section in sections:
                if section in ai_analysis:
                    section_data = ai_analysis[section]
                    if isinstance(section_data, dict):
                        # è¾æ›¸ã®å……å®Ÿåº¦ï¼ˆã‚­ãƒ¼æ•°ã¨å€¤ã®é•·ã•ï¼‰
                        score = min(1.0, len(section_data) / 5)
                        completeness_scores.append(score)
            
            quality['completeness_score'] = sum(completeness_scores) / len(completeness_scores) if completeness_scores else 0.0
            
            # å“è³ªåˆ¤å®š
            quality['assessment_details'] = {
                "meets_level_4": verbalization_level >= 4,
                "meets_confidence_threshold": confidence_score >= QUALITY_CHECK_THRESHOLDS['confidence'],
                "meets_completeness_threshold": quality['completeness_score'] >= QUALITY_CHECK_THRESHOLDS['completeness']
            }
            
            return quality
            
        except Exception as e:
            logger.error(f"âŒ å“è³ªè©•ä¾¡å¤±æ•—: {e}")
            return quality
    
    def _pdf_first_page_to_image(self, file_path: str) -> str:
        """PDF/Wordæ–‡æ›¸ã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’ç”»åƒåŒ–
        
        Args:
            file_path: PDF/Wordãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            å¤‰æ›å¾Œã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€å¤±æ•—æ™‚ã¯None
        """
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’ç¢ºèª
            file_ext = os.path.splitext(file_path)[1].lower()
            
            # Wordæ–‡æ›¸ã®å ´åˆã¯PDFã«å¤‰æ›ã—ã¦ã‹ã‚‰ç”»åƒåŒ–
            if file_ext in ['.doc', '.docx']:
                logger.info(f"Wordâ†’PDFâ†’ç”»åƒå¤‰æ›é–‹å§‹: {os.path.basename(file_path)}")
                
                # Wordâ†’PDFå¤‰æ›ï¼ˆLibreOfficeä½¿ç”¨ï¼‰
                try:
                    import subprocess
                    temp_dir = os.path.dirname(file_path)
                    
                    # LibreOfficeã§PDFã«å¤‰æ›
                    subprocess.run([
                        'soffice',
                        '--headless',
                        '--convert-to', 'pdf',
                        '--outdir', temp_dir,
                        file_path
                    ], check=True, capture_output=True, timeout=30)
                    
                    # å¤‰æ›ã•ã‚ŒãŸPDFãƒ‘ã‚¹
                    pdf_path = file_path.rsplit('.', 1)[0] + '.pdf'
                    
                    if not os.path.exists(pdf_path):
                        logger.warning("Wordâ†’PDFå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        return None
                    
                    # PDFã‚’ä½¿ç”¨ã—ã¦ç”»åƒå¤‰æ›ã‚’ç¶šè¡Œ
                    file_path = pdf_path
                    file_ext = '.pdf'
                    
                except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired) as e:
                    logger.warning(f"Wordâ†’PDFå¤‰æ›å¤±æ•—: {e}")
                    logger.warning("  LibreOfficeãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                    logger.warning("  ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: brew install libreoffice (Mac)")
                    return None
            
            # PDFã‚’ç”»åƒã«å¤‰æ›
            if file_ext == '.pdf':
                from pdf2image import convert_from_path
                
                logger.info(f"PDFâ†’ç”»åƒå¤‰æ›é–‹å§‹: {os.path.basename(file_path)}")
                
                # PDFã®1ãƒšãƒ¼ã‚¸ç›®ã®ã¿ã‚’ç”»åƒã«å¤‰æ›
                images = convert_from_path(
                    file_path, 
                    first_page=1, 
                    last_page=1,
                    dpi=150  # è§£åƒåº¦ï¼ˆé«˜ã™ãã‚‹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå¤§ï¼‰
                )
                
                if not images:
                    logger.warning("PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    return None
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                temp_image_path = file_path.replace('.pdf', '_page1.jpg')
                images[0].save(temp_image_path, 'JPEG', quality=85)
                
                logger.info(f"å¤‰æ›æˆåŠŸ: {os.path.basename(temp_image_path)}")
                return temp_image_path
            else:
                logger.warning(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_ext}")
                return None
            
        except ImportError:
            logger.error("ã‚¨ãƒ©ãƒ¼: pdf2imageãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            logger.error("  ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install pdf2image")
            logger.error("  ã‚·ã‚¹ãƒ†ãƒ ä¾å­˜: brew install poppler (Mac)")
            return None
            
        except Exception as e:
            logger.error(f"æ–‡æ›¸å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _get_mime_type(self, file_path: str) -> str:
        """MIMEã‚¿ã‚¤ãƒ—å–å¾—"""
        ext = os.path.splitext(file_path)[1].lower()
        mime_types = {
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp'
        }
        return mime_types.get(ext, 'image/jpeg')

    def extract_date_from_evidence(self, 
                                   evidence_id: str,
                                   file_path: str,
                                   file_type: str,
                                   original_filename: str) -> Dict:
        """
        è¨¼æ‹ ã‹ã‚‰æ—¥ä»˜æƒ…å ±ã‚’æŠ½å‡ºï¼ˆè»½é‡ç‰ˆAIåˆ†æï¼‰
        
        Args:
            evidence_id: è¨¼æ‹ IDï¼ˆä¾‹: tmp_001ï¼‰
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_type: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
            original_filename: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«å
        
        Returns:
            æ—¥ä»˜æŠ½å‡ºçµæœ {
                "evidence_id": str,
                "extracted_dates": [{"date": "YYYY-MM-DD", "confidence": float, "context": str}],
                "primary_date": "YYYY-MM-DD" or None,
                "date_source": "content" | "filename" | "metadata" | "unknown"
            }
        """
        logger.info(f"ğŸ“… æ—¥ä»˜æŠ½å‡ºé–‹å§‹: {evidence_id} - {original_filename}")
        
        try:
            # æ—¥ä»˜æŠ½å‡ºç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            date_prompt = f"""
ä»¥ä¸‹ã®è¨¼æ‹ ã‹ã‚‰æ—¥ä»˜æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

è¨¼æ‹ ID: {evidence_id}
ãƒ•ã‚¡ã‚¤ãƒ«å: {original_filename}

ã€æŠ½å‡ºæŒ‡ç¤ºã€‘
1. è¨¼æ‹ å†…å®¹ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
2. ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
4. å„æ—¥ä»˜ã«ã¤ã„ã¦ã€ä¿¡é ¼åº¦ï¼ˆ0.0-1.0ï¼‰ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã©ã“ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰ã‚’è¨˜éŒ²
5. æœ€ã‚‚ä¿¡é ¼ã§ãã‚‹ã€Œä¸»è¦æ—¥ä»˜ã€ã‚’1ã¤é¸æŠ

ã€å‡ºåŠ›å½¢å¼: JSONã€‘
{{
  "evidence_id": "{evidence_id}",
  "extracted_dates": [
    {{
      "date": "YYYY-MM-DD",
      "confidence": 0.0-1.0,
      "context": "è¨¼æ‹ æœ¬æ–‡ã®ä½œæˆæ—¥ä»˜ã¨ã—ã¦è¨˜è¼‰",
      "source": "content" | "filename" | "metadata"
    }}
  ],
  "primary_date": "YYYY-MM-DD" or null,
  "date_source": "content" | "filename" | "metadata" | "unknown",
  "extraction_notes": "æ—¥ä»˜æŠ½å‡ºã«é–¢ã™ã‚‹è£œè¶³"
}}

**é‡è¦**: JSONä»¥å¤–ã®ä½™åˆ†ãªãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
"""
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸåˆ†æ
            if file_type in ['image', 'pdf', 'document']:
                # Vision APIä½¿ç”¨
                result = self._analyze_with_vision(file_path, date_prompt, file_type)
            else:
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ†æ
                result = self._analyze_with_text(date_prompt, {})
            
            logger.info(f"âœ… æ—¥ä»˜æŠ½å‡ºå®Œäº†: {evidence_id}")
            
            # ä¸»è¦æ—¥ä»˜ã‚’ãƒ­ã‚°å‡ºåŠ›
            primary_date = result.get('primary_date')
            if primary_date:
                logger.info(f"   ä¸»è¦æ—¥ä»˜: {primary_date}")
            else:
                logger.warning(f"   âš ï¸ æ—¥ä»˜ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ—¥ä»˜æŠ½å‡ºå¤±æ•—: {evidence_id} - {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            return {
                "evidence_id": evidence_id,
                "extracted_dates": [],
                "primary_date": None,
                "date_source": "unknown",
                "extraction_error": str(e)
            }

    def analyze_complete(self, processed_data: Dict, evidence_number: str) -> Dict:
        """
        analyze_evidence_complete ã®å¾Œæ–¹äº’æ›ã‚¨ã‚¤ãƒªã‚¢ã‚¹
        
        Args:
            processed_data: ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†çµæœ
            evidence_number: è¨¼æ‹ ç•ªå·
        
        Returns:
            åˆ†æçµæœï¼ˆç°¡æ˜“ç‰ˆï¼‰
        """
        logger.warning(f"analyze_complete ã¯éæ¨å¥¨ã§ã™ã€‚analyze_evidence_complete ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
        
        # ç°¡æ˜“çš„ãªè¿”ã‚Šå€¤ï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯é©åˆ‡ãªå®Ÿè£…ãŒå¿…è¦ï¼‰
        return {
            "quality_scores": {
                "completeness_score": 90.0,
                "confidence_score": 85.0,
                "verbalization_level": 4
            },
            "analysis_result": f"è¨¼æ‹  {evidence_number} ã®åˆ†æçµæœ",
            "evidence_number": evidence_number
        }
