#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

é‡è¤‡ã‚¨ãƒ³ãƒˆãƒªã®æ¤œå‡ºã¨ãƒãƒ¼ã‚¸ã‚’è¡Œã„ã¾ã™ã€‚
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)


class DatabaseCleanup:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, database_path: str):
        """åˆæœŸåŒ–
        
        Args:
            database_path: database.jsonã®ãƒ‘ã‚¹
        """
        self.database_path = Path(database_path)
        self.database = self._load_database()
    
    def _load_database(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰"""
        if not self.database_path.exists():
            logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.database_path}")
            return {"evidence": []}
        
        with open(self.database_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _save_database(self, backup: bool = True) -> None:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜
        
        Args:
            backup: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹ã‹
        """
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
        if backup:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = self.database_path.parent / f"database_backup_{timestamp}.json"
            
            with open(self.database_path, 'r', encoding='utf-8') as f:
                backup_data = f.read()
            
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(backup_data)
            
            logger.info(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
        
        # ä¿å­˜
        with open(self.database_path, 'w', encoding='utf-8') as f:
            json.dump(self.database, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜: {self.database_path}")
    
    def find_duplicates(self) -> List[List[Dict]]:
        """é‡è¤‡ã‚¨ãƒ³ãƒˆãƒªã‚’æ¤œå‡º
        
        Returns:
            é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒªã‚¹ãƒˆï¼ˆå„ã‚°ãƒ«ãƒ¼ãƒ—ã¯é‡è¤‡ã™ã‚‹ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆï¼‰
        """
        evidence_list = self.database.get("evidence", [])
        
        # è­˜åˆ¥å­ã®çµ„ã¿åˆã‚ã›ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        groups = {}
        
        for idx, entry in enumerate(evidence_list):
            # è­˜åˆ¥å­ã®çµ„ã¿åˆã‚ã›ã‚’ä½œæˆ
            # temp_id, evidence_id, evidence_number ã®ã„ãšã‚Œã‹ãŒä¸€è‡´ã—ãŸã‚‰ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            identifiers = []
            
            if entry.get('temp_id'):
                identifiers.append(('temp_id', entry['temp_id']))
            if entry.get('evidence_id'):
                identifiers.append(('evidence_id', entry['evidence_id']))
            if entry.get('evidence_number'):
                identifiers.append(('evidence_number', entry['evidence_number']))
            
            # ã„ãšã‚Œã‹ã®è­˜åˆ¥å­ãŒæ—¢å­˜ã®ã‚°ãƒ«ãƒ¼ãƒ—ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
            matched_group = None
            for group_key in list(groups.keys()):
                for id_type, id_value in identifiers:
                    if (id_type, id_value) in group_key:
                        matched_group = group_key
                        break
                if matched_group:
                    break
            
            # æ—¢å­˜ã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ ã¾ãŸã¯æ–°è¦ã‚°ãƒ«ãƒ¼ãƒ—ä½œæˆ
            if matched_group:
                groups[matched_group].append((idx, entry))
            else:
                # æ–°è¦ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚­ãƒ¼ã¯è­˜åˆ¥å­ã®ã‚¿ãƒ—ãƒ«
                new_key = tuple(identifiers)
                groups[new_key] = [(idx, entry)]
        
        # é‡è¤‡ï¼ˆ2ã¤ä»¥ä¸Šã®ã‚¨ãƒ³ãƒˆãƒªãŒã‚ã‚‹ï¼‰ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æŠ½å‡º
        duplicates = []
        for key, entries in groups.items():
            if len(entries) > 1:
                # å®Œå…¨ã«åŒä¸€ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯é™¤å¤–ï¼ˆå‚ç…§ãŒç•°ãªã‚‹åˆ¥ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚«ã‚¦ãƒ³ãƒˆï¼‰
                unique_entries = []
                seen_ids = set()
                
                for idx, entry in entries:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ä¸€æ„æ€§ã‚’åˆ¤å®š
                    if idx not in seen_ids:
                        unique_entries.append(entry)
                        seen_ids.add(idx)
                
                if len(unique_entries) > 1:
                    duplicates.append(unique_entries)
        
        return duplicates
    
    def analyze_duplicates(self) -> None:
        """é‡è¤‡ã‚¨ãƒ³ãƒˆãƒªã‚’åˆ†æã—ã¦è¡¨ç¤º"""
        duplicates = self.find_duplicates()
        
        if not duplicates:
            logger.info("âœ… é‡è¤‡ã‚¨ãƒ³ãƒˆãƒªã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        logger.info(f"\nâš ï¸  {len(duplicates)}å€‹ã®é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ\n")
        
        for i, group in enumerate(duplicates, 1):
            logger.info(f"ã€é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ— {i}ã€‘")
            
            for j, entry in enumerate(group, 1):
                temp_id = entry.get('temp_id', 'ãªã—')
                evidence_id = entry.get('evidence_id', 'ãªã—')
                evidence_number = entry.get('evidence_number', 'ãªã—')
                file_name = entry.get('file_name', 'ãªã—')
                has_analysis = 'phase1_complete_analysis' in entry
                
                logger.info(f"  ã‚¨ãƒ³ãƒˆãƒª {j}:")
                logger.info(f"    temp_id: {temp_id}")
                logger.info(f"    evidence_id: {evidence_id}")
                logger.info(f"    evidence_number: {evidence_number}")
                logger.info(f"    file_name: {file_name}")
                logger.info(f"    åˆ†æå®Œäº†: {'âœ…' if has_analysis else 'âŒ'}")
            
            logger.info("")
    
    def merge_duplicates(self, dry_run: bool = True) -> None:
        """é‡è¤‡ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒãƒ¼ã‚¸
        
        Args:
            dry_run: True ã®å ´åˆã¯å®Ÿéš›ã«ã¯å¤‰æ›´ã›ãšã€å¤‰æ›´å†…å®¹ã®ã¿è¡¨ç¤º
        """
        duplicates = self.find_duplicates()
        
        if not duplicates:
            logger.info("âœ… é‡è¤‡ã‚¨ãƒ³ãƒˆãƒªã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return
        
        logger.info(f"\nğŸ”§ {len(duplicates)}å€‹ã®é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ãƒãƒ¼ã‚¸ã—ã¾ã™\n")
        
        evidence_list = self.database.get("evidence", [])
        indices_to_remove = set()
        
        for group in duplicates:
            # åˆ†æå®Œäº†æ¸ˆã¿ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å„ªå…ˆ
            analyzed = [e for e in group if 'phase1_complete_analysis' in e]
            unanalyzed = [e for e in group if 'phase1_complete_analysis' not in e]
            
            if analyzed:
                # åˆ†æå®Œäº†æ¸ˆã¿ã‚¨ãƒ³ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹å ´åˆ
                primary = analyzed[0]  # æœ€åˆã®åˆ†æå®Œäº†ã‚¨ãƒ³ãƒˆãƒªã‚’ä½¿ç”¨
                
                # temp_idã‚’æœªåˆ†æã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰å–å¾—ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
                for unanalyzed_entry in unanalyzed:
                    if unanalyzed_entry.get('temp_id') and not primary.get('temp_id'):
                        primary['temp_id'] = unanalyzed_entry['temp_id']
                    if unanalyzed_entry.get('temp_number') and not primary.get('temp_number'):
                        primary['temp_number'] = unanalyzed_entry['temp_number']
                
                # å‰Šé™¤å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
                for entry in group:
                    if entry is not primary:
                        try:
                            idx = evidence_list.index(entry)
                            indices_to_remove.add(idx)
                        except ValueError:
                            pass
                
                logger.info(f"âœ… ãƒãƒ¼ã‚¸: {primary.get('evidence_id', primary.get('temp_id'))}")
                logger.info(f"   ä¿æŒ: {primary.get('file_name')}")
                logger.info(f"   å‰Šé™¤: {len(group) - 1}å€‹ã®ã‚¨ãƒ³ãƒˆãƒª")
            else:
                # ã™ã¹ã¦æœªåˆ†æã®å ´åˆã¯æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä¿æŒ
                primary = group[0]
                
                for entry in group[1:]:
                    try:
                        idx = evidence_list.index(entry)
                        indices_to_remove.add(idx)
                    except ValueError:
                        pass
                
                logger.info(f"âš ï¸  ãƒãƒ¼ã‚¸ï¼ˆæœªåˆ†æï¼‰: {primary.get('temp_id', primary.get('evidence_id'))}")
        
        # å‰Šé™¤ã‚’å®Ÿè¡Œï¼ˆdry_runã§ãªã„å ´åˆï¼‰
        if not dry_run:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é™é †ã«ã‚½ãƒ¼ãƒˆã—ã¦å‰Šé™¤ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãšã‚Œã‚’é˜²ãï¼‰
            for idx in sorted(indices_to_remove, reverse=True):
                del evidence_list[idx]
            
            self.database["evidence"] = evidence_list
            self._save_database(backup=True)
            
            logger.info(f"\nâœ… {len(indices_to_remove)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            logger.info(f"   æ®‹ã‚Š: {len(evidence_list)}å€‹ã®ã‚¨ãƒ³ãƒˆãƒª")
        else:
            logger.info(f"\nğŸ’¡ ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†ï¼ˆå®Ÿéš›ã®å¤‰æ›´ã¯è¡Œã‚ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰")
            logger.info(f"   å®Ÿè¡Œã™ã‚‹å ´åˆã¯ dry_run=False ã§å®Ÿè¡Œã—ã¦ãã ã•ã„")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£")
    parser.add_argument("database", help="database.jsonã®ãƒ‘ã‚¹")
    parser.add_argument("--analyze", action="store_true", help="é‡è¤‡ã‚’åˆ†æã—ã¦è¡¨ç¤º")
    parser.add_argument("--merge", action="store_true", help="é‡è¤‡ã‚’ãƒãƒ¼ã‚¸")
    parser.add_argument("--execute", action="store_true", help="å®Ÿéš›ã«å¤‰æ›´ã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼‰")
    
    args = parser.parse_args()
    
    cleanup = DatabaseCleanup(args.database)
    
    if args.analyze:
        cleanup.analyze_duplicates()
    
    if args.merge:
        dry_run = not args.execute
        cleanup.merge_duplicates(dry_run=dry_run)
    
    if not args.analyze and not args.merge:
        parser.print_help()


if __name__ == "__main__":
    main()
