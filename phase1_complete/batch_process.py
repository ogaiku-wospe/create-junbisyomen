#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 1å®Œå…¨ç‰ˆã‚·ã‚¹ãƒ†ãƒ  - ä¸€æ‹¬å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã€ä½¿ç”¨æ–¹æ³•ã€‘
    # è¨¼æ‹ ç•ªå·ã®ãƒªã‚¹ãƒˆã§ä¸€æ‹¬å‡¦ç†
    python3 batch_process.py --evidence ko70 ko71 ko72 ko73
    
    # è¨¼æ‹ ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ä¸€æ‹¬å‡¦ç†
    python3 batch_process.py --directory /path/to/evidence_files/
    
    # ç¯„å›²æŒ‡å®šã§ä¸€æ‹¬å‡¦ç†
    python3 batch_process.py --range ko70-73
    
    # æœªå‡¦ç†ã®è¨¼æ‹ ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦ä¸€æ‹¬å‡¦ç†
    python3 batch_process.py --auto

ã€æ©Ÿèƒ½ã€‘
    - è¤‡æ•°è¨¼æ‹ ã®ä¸€æ‹¬å‡¦ç†
    - é€²æ—è¡¨ç¤ºãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    - è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
    - è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import os
import sys
import json
import logging
import argparse
from datetime import datetime
from typing import List, Dict
import time

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from config import *
    from metadata_extractor import MetadataExtractor
    from file_processor import FileProcessor
    from ai_analyzer_complete import AIAnalyzerComplete
except ImportError as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    sys.exit(1)

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('batch_process.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class BatchProcessor:
    """ä¸€æ‹¬å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, database_path="database.json"):
        """åˆæœŸåŒ–"""
        self.database_path = database_path
        self.metadata_extractor = MetadataExtractor()
        self.file_processor = FileProcessor()
        self.ai_analyzer = AIAnalyzerComplete()
        
        self.success_count = 0
        self.failed_count = 0
        self.skipped_count = 0
        self.results = []
    
    def load_database(self) -> dict:
        """database.jsonã®èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.database_path):
            with open(self.database_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            logger.warning(f"âš ï¸ {self.database_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
            return {
                "case_info": {
                    "case_name": CASE_NAME,
                    "plaintiff": PLAINTIFF,
                    "defendant": DEFENDANT,
                    "court": COURT
                },
                "evidence": [],
                "metadata": {
                    "version": "3.0",
                    "created_at": datetime.now().isoformat(),
                    "last_updated": datetime.now().isoformat()
                }
            }
    
    def save_database(self, database: dict):
        """database.jsonã®ä¿å­˜"""
        database["metadata"]["last_updated"] = datetime.now().isoformat()
        database["metadata"]["total_evidence_count"] = len(database["evidence"])
        database["metadata"]["completed_count"] = len([e for e in database["evidence"] if e.get('status') == 'completed'])
        
        with open(self.database_path, 'w', encoding='utf-8') as f:
            json.dump(database, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ database.jsonã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    
    def is_evidence_completed(self, database: dict, evidence_number: str) -> bool:
        """è¨¼æ‹ ãŒæ—¢ã«å®Œäº†ã—ã¦ã„ã‚‹ã‹ç¢ºèª"""
        for evidence in database["evidence"]:
            if evidence.get("evidence_number") == evidence_number:
                if evidence.get("status") == "completed":
                    return True
        return False
    
    def process_evidence(
        self, 
        evidence_number: str, 
        file_path: str,
        skip_completed: bool = True
    ) -> Dict:
        """è¨¼æ‹ ã®å‡¦ç†
        
        Args:
            evidence_number: è¨¼æ‹ ç•ªå·
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            skip_completed: å®Œäº†æ¸ˆã¿è¨¼æ‹ ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹
            
        Returns:
            å‡¦ç†çµæœã®è¾æ›¸
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"  è¨¼æ‹  {evidence_number} ã®å‡¦ç†é–‹å§‹")
        logger.info(f"{'='*70}")
        
        result = {
            "evidence_number": evidence_number,
            "file_path": file_path,
            "status": "unknown",
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "error": None
        }
        
        try:
            # database.jsonã‚’èª­ã¿è¾¼ã¿
            database = self.load_database()
            
            # å®Œäº†æ¸ˆã¿ãƒã‚§ãƒƒã‚¯
            if skip_completed and self.is_evidence_completed(database, evidence_number):
                logger.info(f"â­ï¸  è¨¼æ‹  {evidence_number} ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                result["status"] = "skipped"
                result["end_time"] = datetime.now().isoformat()
                self.skipped_count += 1
                return result
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            if not os.path.exists(file_path):
                logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
                result["status"] = "failed"
                result["error"] = f"File not found: {file_path}"
                result["end_time"] = datetime.now().isoformat()
                self.failed_count += 1
                return result
            
            logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}")
            
            # 1. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            logger.info(f"ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­...")
            metadata = self.metadata_extractor.extract_complete_metadata(file_path)
            logger.info(f"  âœ… SHA-256: {metadata['file_hash']['sha256'][:16]}...")
            logger.info(f"  âœ… ã‚µã‚¤ã‚º: {metadata['file_size_bytes'] / 1024:.2f} KB")
            
            # 2. ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            logger.info(f"ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
            processed_data = self.file_processor.process_file(file_path, evidence_number)
            logger.info(f"  âœ… ã‚¿ã‚¤ãƒ—: {processed_data['file_type']}")
            logger.info(f"  âœ… æŠ½å‡ºç”»åƒæ•°: {len(processed_data.get('images', []))}")
            
            # 3. AIåˆ†æ
            logger.info(f"ğŸ¤– AIåˆ†æã‚’å®Ÿè¡Œä¸­ï¼ˆGPT-4o Visionï¼‰...")
            analysis_result = self.ai_analyzer.analyze_complete(  # TODO: analyze_evidence_complete ã¸ã®ç§»è¡Œã‚’æ¤œè¨  # TODO: analyze_evidence_complete ã¸ã®ç§»è¡Œã‚’æ¤œè¨
                processed_data=processed_data,
                evidence_number=evidence_number
            )
            
            # 4. å“è³ªè©•ä¾¡
            quality_scores = analysis_result['quality_scores']
            logger.info(f"ğŸ“ˆ å“è³ªè©•ä¾¡:")
            logger.info(f"  âœ… å®Œå…¨æ€§: {quality_scores['completeness_score']:.1f}%")
            logger.info(f"  âœ… ä¿¡é ¼åº¦: {quality_scores['confidence_score']:.1f}%")
            logger.info(f"  âœ… è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«: {quality_scores['verbalization_level']}")
            
            # 5. database.jsonã«ä¿å­˜
            logger.info(f"ğŸ’¾ database.jsonã«ä¿å­˜ä¸­...")
            
            evidence_entry = {
                "evidence_number": evidence_number,
                "complete_metadata": metadata,
                "phase1_complete_analysis": analysis_result,
                "status": "completed",
                "processed_at": datetime.now().isoformat()
            }
            
            # æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã®æ›´æ–°ã¾ãŸã¯æ–°è¦è¿½åŠ 
            existing_index = next(
                (i for i, e in enumerate(database["evidence"]) 
                 if e.get("evidence_number") == evidence_number),
                None
            )
            
            if existing_index is not None:
                database["evidence"][existing_index] = evidence_entry
                logger.info(f"  âœ… æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã‚’æ›´æ–°")
            else:
                database["evidence"].append(evidence_entry)
                logger.info(f"  âœ… æ–°è¦ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ")
            
            self.save_database(database)
            
            logger.info(f"\nâœ… è¨¼æ‹  {evidence_number} ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            
            result["status"] = "success"
            result["quality_scores"] = quality_scores
            result["end_time"] = datetime.now().isoformat()
            self.success_count += 1
            
        except Exception as e:
            logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
            result["status"] = "failed"
            result["error"] = str(e)
            result["end_time"] = datetime.now().isoformat()
            self.failed_count += 1
        
        self.results.append(result)
        return result
    
    def process_batch(
        self, 
        evidence_list: List[Dict[str, str]],
        skip_completed: bool = True,
        retry_failed: bool = True
    ):
        """ä¸€æ‹¬å‡¦ç†
        
        Args:
            evidence_list: è¨¼æ‹ æƒ…å ±ã®ãƒªã‚¹ãƒˆ [{"number": "ko70", "file": "/path/to/file"}, ...]
            skip_completed: å®Œäº†æ¸ˆã¿è¨¼æ‹ ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹
            retry_failed: å¤±æ•—ã—ãŸè¨¼æ‹ ã‚’å†è©¦è¡Œã™ã‚‹ã‹
        """
        logger.info(f"\n{'='*70}")
        logger.info(f"  ä¸€æ‹¬å‡¦ç†é–‹å§‹")
        logger.info(f"  å‡¦ç†å¯¾è±¡: {len(evidence_list)}ä»¶")
        logger.info(f"{'='*70}\n")
        
        start_time = datetime.now()
        
        for idx, evidence in enumerate(evidence_list, 1):
            evidence_number = evidence["number"]
            file_path = evidence["file"]
            
            logger.info(f"\n[{idx}/{len(evidence_list)}] å‡¦ç†ä¸­: {evidence_number}")
            
            result = self.process_evidence(
                evidence_number=evidence_number,
                file_path=file_path,
                skip_completed=skip_completed
            )
            
            # é€²æ—è¡¨ç¤º
            logger.info(f"\nğŸ“Š ç¾åœ¨ã®é€²æ—:")
            logger.info(f"  âœ… æˆåŠŸ: {self.success_count}")
            logger.info(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {self.skipped_count}")
            logger.info(f"  âŒ å¤±æ•—: {self.failed_count}")
            logger.info(f"  ğŸ“ˆ é€²æ—ç‡: {idx}/{len(evidence_list)} ({idx/len(evidence_list)*100:.1f}%)")
            
            # å°‘ã—å¾…æ©Ÿï¼ˆAPIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰
            if idx < len(evidence_list):
                time.sleep(2)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        self.print_final_report(duration)
    
    def print_final_report(self, duration):
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º"""
        logger.info(f"\n{'='*70}")
        logger.info(f"  ä¸€æ‹¬å‡¦ç†å®Œäº†")
        logger.info(f"{'='*70}")
        
        logger.info(f"\nğŸ“Š å‡¦ç†çµæœ:")
        logger.info(f"  âœ… æˆåŠŸ: {self.success_count}")
        logger.info(f"  â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {self.skipped_count}")
        logger.info(f"  âŒ å¤±æ•—: {self.failed_count}")
        logger.info(f"  ğŸ“Š åˆè¨ˆ: {len(self.results)}")
        
        logger.info(f"\nâ±ï¸  å‡¦ç†æ™‚é–“:")
        logger.info(f"  - ç·å‡¦ç†æ™‚é–“: {duration}")
        if self.success_count > 0:
            avg_time = duration.total_seconds() / self.success_count
            logger.info(f"  - å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.1f}ç§’/ä»¶")
        
        # å¤±æ•—ã—ãŸè¨¼æ‹ ã®ãƒªã‚¹ãƒˆ
        if self.failed_count > 0:
            logger.info(f"\nâŒ å¤±æ•—ã—ãŸè¨¼æ‹ :")
            for result in self.results:
                if result["status"] == "failed":
                    logger.info(f"  - {result['evidence_number']}: {result.get('error', 'Unknown error')}")
        
        logger.info(f"\n{'='*70}\n")
    
    def get_unprocessed_evidence(self, database: dict) -> List[str]:
        """æœªå‡¦ç†ã®è¨¼æ‹ ç•ªå·ã‚’å–å¾—"""
        completed_numbers = {e.get("evidence_number") for e in database["evidence"] if e.get("status") == "completed"}
        
        # ã“ã“ã§ã¯æ‰‹å‹•ã§æœªå‡¦ç†ã®è¨¼æ‹ ç•ªå·ã‚’æŒ‡å®š
        # å®Ÿéš›ã®Google Driveæ¤œç´¢æ©Ÿèƒ½ã¯åˆ¥é€”å®Ÿè£…ãŒå¿…è¦
        all_evidence = ["ko70", "ko71", "ko72", "ko73"]  # ä¾‹
        
        unprocessed = [e for e in all_evidence if e not in completed_numbers]
        return unprocessed


def parse_evidence_range(range_str: str) -> List[str]:
    """è¨¼æ‹ ç•ªå·ã®ç¯„å›²ã‚’è§£æ
    
    Args:
        range_str: ç¯„å›²æ–‡å­—åˆ—ï¼ˆä¾‹: "ko70-73"ï¼‰
        
    Returns:
        è¨¼æ‹ ç•ªå·ã®ãƒªã‚¹ãƒˆ
    """
    if '-' not in range_str:
        return [range_str]
    
    try:
        prefix = range_str.split('-')[0].rstrip('0123456789')
        start = int(range_str.split('-')[0][len(prefix):])
        end = int(range_str.split('-')[1])
        return [f"{prefix}{i}" for i in range(start, end + 1)]
    except ValueError:
        logger.error(f"âŒ ç¯„å›²æŒ‡å®šã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: {range_str}")
        return []


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='Phase 1å®Œå…¨ç‰ˆã‚·ã‚¹ãƒ†ãƒ  - ä¸€æ‹¬å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # è¨¼æ‹ æŒ‡å®šæ–¹æ³•
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        '--evidence',
        nargs='+',
        help='è¨¼æ‹ ç•ªå·ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: ko70 ko71 ko72ï¼‰'
    )
    group.add_argument(
        '--range',
        type=str,
        help='è¨¼æ‹ ç•ªå·ã®ç¯„å›²ï¼ˆä¾‹: ko70-73ï¼‰'
    )
    group.add_argument(
        '--auto',
        action='store_true',
        help='æœªå‡¦ç†ã®è¨¼æ‹ ã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å‡¦ç†'
    )
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æŒ‡å®šæ–¹æ³•
    parser.add_argument(
        '--directory',
        type=str,
        help='è¨¼æ‹ ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
    )
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        '--skip-completed',
        action='store_true',
        default=True,
        help='å®Œäº†æ¸ˆã¿è¨¼æ‹ ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='database.json',
        help='å‡ºåŠ›å…ˆdatabase.jsonã®ãƒ‘ã‚¹'
    )
    
    args = parser.parse_args()
    
    # ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    if not os.getenv('OPENAI_API_KEY'):
        logger.error("âŒ ã‚¨ãƒ©ãƒ¼: OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return 1
    
    # è¨¼æ‹ ç•ªå·ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    if args.evidence:
        evidence_numbers = args.evidence
    elif args.range:
        evidence_numbers = parse_evidence_range(args.range)
    elif args.auto:
        processor = BatchProcessor(args.output)
        database = processor.load_database()
        evidence_numbers = processor.get_unprocessed_evidence(database)
        logger.info(f"ğŸ“‹ æœªå‡¦ç†ã®è¨¼æ‹ ã‚’æ¤œå‡ºã—ã¾ã—ãŸ: {evidence_numbers}")
    
    if not evidence_numbers:
        logger.error("âŒ å‡¦ç†ã™ã‚‹è¨¼æ‹ ãŒã‚ã‚Šã¾ã›ã‚“")
        return 1
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    evidence_list = []
    for number in evidence_numbers:
        if args.directory:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰è¨¼æ‹ ç•ªå·ã«å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            file_path = None
            for ext in ['.pdf', '.jpg', '.jpeg', '.png', '.docx', '.mp4', '.mp3']:
                candidate = os.path.join(args.directory, f"{number}{ext}")
                if os.path.exists(candidate):
                    file_path = candidate
                    break
            
            if not file_path:
                logger.warning(f"âš ï¸ {number} ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                continue
        else:
            logger.error(f"âŒ --directory ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™")
            return 1
        
        evidence_list.append({
            "number": number,
            "file": file_path
        })
    
    if not evidence_list:
        logger.error("âŒ å‡¦ç†å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return 1
    
    # ä¸€æ‹¬å‡¦ç†å®Ÿè¡Œ
    processor = BatchProcessor(args.output)
    processor.process_batch(
        evidence_list=evidence_list,
        skip_completed=args.skip_completed
    )
    
    return 0 if processor.failed_count == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
