#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 1å®Œå…¨ç‰ˆã‚·ã‚¹ãƒ†ãƒ  - ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æèµ·å‰_åèª‰æ¯€æç­‰æå®³è³ å„Ÿè«‹æ±‚äº‹ä»¶ è¨¼æ‹ åˆ†æã‚·ã‚¹ãƒ†ãƒ 

ä½¿ç”¨æ–¹æ³•:
    # å˜ä¸€è¨¼æ‹ ã®åˆ†æ
    python phase1_cli.py ko70
    
    # è¤‡æ•°è¨¼æ‹ ã®åˆ†æ
    python phase1_cli.py ko70 ko71 ko72 ko73
    
    # ç¯„å›²æŒ‡å®šï¼ˆæœªå®Ÿè£…ï¼‰
    python phase1_cli.py --range ko70-73
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ç›´æ¥æŒ‡å®š
    python phase1_cli.py --file /path/to/evidence.pdf --number ko70
    
    # è©³ç´°ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    python phase1_cli.py ko70 --verbose --output custom_database.json
"""

import argparse
import os
import sys
import json
import logging
from datetime import datetime
from typing import List

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from config import *
    from metadata_extractor import MetadataExtractor
    from file_processor import FileProcessor
    from ai_analyzer_complete import AIAnalyzerComplete
except ImportError as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    sys.exit(1)


def setup_logging(verbose: bool = False):
    """ãƒ­ã‚®ãƒ³ã‚°è¨­å®š"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler('phase1_cli.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )


def load_database(path: str) -> dict:
    """database.jsonã®èª­ã¿è¾¼ã¿"""
    if os.path.exists(path):
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        return {
            "case_info": {
                "case_name": "æèµ·å‰_åèª‰æ¯€æç­‰æå®³è³ å„Ÿè«‹æ±‚äº‹ä»¶",
                "plaintiff": "å°åŸç³ï¼ˆã—ã‚ãã¾ã‚¯ãƒ©ãƒ•ãƒˆï¼‰",
                "defendant": "çŸ³æ‘ã¾ã‚†ã‹ï¼ˆSUBÃ—MISSIONï¼‰",
                "court": "æ±äº¬åœ°æ–¹è£åˆ¤æ‰€"
            },
            "evidence": [],
            "metadata": {
                "version": "3.0",
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat()
            }
        }


def save_database(database: dict, path: str):
    """database.jsonã®ä¿å­˜"""
    database["metadata"]["last_updated"] = datetime.now().isoformat()
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(database, f, ensure_ascii=False, indent=2)


def process_evidence(
    evidence_number: str,
    file_path: str = None,
    database_path: str = "database.json"
) -> bool:
    """è¨¼æ‹ ã®å‡¦ç†
    
    Args:
        evidence_number: è¨¼æ‹ ç•ªå·ï¼ˆä¾‹: ko70ï¼‰
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯Google Driveã‹ã‚‰æ¤œç´¢ï¼‰
        database_path: database.jsonã®ãƒ‘ã‚¹
        
    Returns:
        å‡¦ç†æˆåŠŸ: True, å¤±æ•—: False
    """
    logger = logging.getLogger(__name__)
    logger.info(f"\n{'='*60}")
    logger.info(f"  è¨¼æ‹  {evidence_number} ã®å‡¦ç†é–‹å§‹")
    logger.info(f"{'='*60}")
    
    try:
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆæœŸåŒ–
        metadata_extractor = MetadataExtractor()
        file_processor = FileProcessor()
        ai_analyzer = AIAnalyzerComplete()
        
        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å–å¾—
        if not file_path:
            logger.info(f"ğŸ” Google Driveã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­...")
            # â€»Google Driveæ¤œç´¢æ©Ÿèƒ½ã¯åˆ¥é€”å®Ÿè£…ãŒå¿…è¦
            logger.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            logger.info("ğŸ’¡ --file ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            return False
        
        if not os.path.exists(file_path):
            logger.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return False
        
        logger.info(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}")
        
        # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        logger.info(f"ğŸ“Š ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºä¸­...")
        metadata = metadata_extractor.extract_complete_metadata(file_path)
        logger.info(f"  âœ… SHA-256: {metadata['file_hash']['sha256'][:16]}...")
        logger.info(f"  âœ… ã‚µã‚¤ã‚º: {metadata['file_size_bytes'] / 1024:.2f} KB")
        
        # 3. ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        logger.info(f"ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
        processed_data = file_processor.process_file(file_path, evidence_number)
        logger.info(f"  âœ… ã‚¿ã‚¤ãƒ—: {processed_data['file_type']}")
        logger.info(f"  âœ… æŠ½å‡ºç”»åƒæ•°: {len(processed_data.get('images', []))}")
        
        # 4. AIåˆ†æ
        logger.info(f"ğŸ¤– AIåˆ†æã‚’å®Ÿè¡Œä¸­ï¼ˆGPT-4o Visionï¼‰...")
        analysis_result = ai_analyzer.analyze_complete(  # TODO: analyze_evidence_complete ã¸ã®ç§»è¡Œã‚’æ¤œè¨  # TODO: analyze_evidence_complete ã¸ã®ç§»è¡Œã‚’æ¤œè¨
            processed_data=processed_data,
            evidence_number=evidence_number
        )
        
        # 5. å“è³ªè©•ä¾¡
        logger.info(f"ğŸ“ˆ å“è³ªè©•ä¾¡:")
        logger.info(f"  âœ… å®Œå…¨æ€§: {analysis_result['quality_scores']['completeness_score']:.1f}%")
        logger.info(f"  âœ… ä¿¡é ¼åº¦: {analysis_result['quality_scores']['confidence_score']:.1f}%")
        logger.info(f"  âœ… è¨€èªåŒ–ãƒ¬ãƒ™ãƒ«: {analysis_result['quality_scores']['verbalization_level']}")
        
        # 6. database.jsonã«ä¿å­˜
        logger.info(f"ğŸ’¾ database.jsonã«ä¿å­˜ä¸­...")
        database = load_database(database_path)
        
        evidence_entry = {
            "evidence_number": evidence_number,
            "complete_metadata": metadata,
            "phase1_complete_analysis": analysis_result,
            "status": "completed",
            "processed_at": datetime.now().isoformat()
        }
        
        # æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã®æ›´æ–°ã¾ãŸã¯æ–°è¦è¿½åŠ 
        existing_index = next(
            (i for i, e in enumerate(database["evidence"]) 
             if e.get("evidence_number") == evidence_number),
            None
        )
        
        if existing_index is not None:
            database["evidence"][existing_index] = evidence_entry
            logger.info(f"  âœ… æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã‚’æ›´æ–°")
        else:
            database["evidence"].append(evidence_entry)
            logger.info(f"  âœ… æ–°è¦ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ")
        
        save_database(database, database_path)
        
        logger.info(f"\nâœ… è¨¼æ‹  {evidence_number} ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='Phase 1å®Œå…¨ç‰ˆã‚·ã‚¹ãƒ†ãƒ  - ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ä½¿ç”¨ä¾‹:
  # å˜ä¸€è¨¼æ‹ ã®åˆ†æï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æŒ‡å®šï¼‰
  python phase1_cli.py ko70 --file /path/to/ko70.pdf
  
  # è¤‡æ•°è¨¼æ‹ ã®åˆ†æ
  python phase1_cli.py ko70 ko71 ko72 --file-dir /path/to/evidence_files/
  
  # è©³ç´°ãƒ­ã‚°å‡ºåŠ›
  python phase1_cli.py ko70 --file /path/to/ko70.pdf --verbose
  
  # ã‚«ã‚¹ã‚¿ãƒ database.json
  python phase1_cli.py ko70 --file /path/to/ko70.pdf --output custom_db.json
        '''
    )
    
    parser.add_argument(
        'evidence_numbers',
        nargs='+',
        help='è¨¼æ‹ ç•ªå·ï¼ˆä¾‹: ko70 ko71 ko72ï¼‰'
    )
    
    parser.add_argument(
        '--file',
        type=str,
        help='è¨¼æ‹ ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆå˜ä¸€è¨¼æ‹ ã®å ´åˆï¼‰'
    )
    
    parser.add_argument(
        '--file-dir',
        type=str,
        help='è¨¼æ‹ ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆè¤‡æ•°è¨¼æ‹ ã®å ´åˆï¼‰'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='database.json',
        help='å‡ºåŠ›å…ˆdatabase.jsonã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: database.jsonï¼‰'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è©³ç´°ãƒ­ã‚°ã‚’å‡ºåŠ›'
    )
    
    args = parser.parse_args()
    
    # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    setup_logging(args.verbose)
    logger = logging.getLogger(__name__)
    
    # ç’°å¢ƒãƒã‚§ãƒƒã‚¯
    if not os.getenv('OPENAI_API_KEY'):
        logger.error("âŒ ã‚¨ãƒ©ãƒ¼: OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        logger.info("export OPENAI_API_KEY='sk-your-api-key' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return 1
    
    # å‡¦ç†é–‹å§‹
    logger.info("="*60)
    logger.info("  Phase 1å®Œå…¨ç‰ˆã‚·ã‚¹ãƒ†ãƒ  - CLIå®Ÿè¡Œ")
    logger.info("="*60)
    
    success_count = 0
    failed_count = 0
    
    for evidence_number in args.evidence_numbers:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ±ºå®š
        if args.file:
            file_path = args.file
        elif args.file_dir:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰è¨¼æ‹ ç•ªå·ã«å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            file_path = None
            for ext in ['.pdf', '.jpg', '.jpeg', '.png', '.docx', '.mp4', '.mp3']:
                candidate = os.path.join(args.file_dir, f"{evidence_number}{ext}")
                if os.path.exists(candidate):
                    file_path = candidate
                    break
            if not file_path:
                logger.warning(f"âš ï¸ {evidence_number} ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                failed_count += 1
                continue
        else:
            logger.error("âŒ --file ã¾ãŸã¯ --file-dir ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™")
            return 1
        
        # å‡¦ç†å®Ÿè¡Œ
        if process_evidence(evidence_number, file_path, args.output):
            success_count += 1
        else:
            failed_count += 1
    
    # çµæœã‚µãƒãƒªãƒ¼
    logger.info("\n" + "="*60)
    logger.info("  å‡¦ç†çµæœã‚µãƒãƒªãƒ¼")
    logger.info("="*60)
    logger.info(f"âœ… æˆåŠŸ: {success_count}")
    logger.info(f"âŒ å¤±æ•—: {failed_count}")
    logger.info(f"ğŸ“Š åˆè¨ˆ: {success_count + failed_count}")
    logger.info("="*60)
    
    return 0 if failed_count == 0 else 1


if __name__ == "__main__":
    sys.exit(main())
