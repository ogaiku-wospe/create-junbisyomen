"""
æ®µéšçš„AIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³
JSONã‚’ä¸€åº¦ã«ç”Ÿæˆã›ãšã€ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å±€æ‰€çš„ãƒ»æŒç¶šçš„ã«ç”Ÿæˆã™ã‚‹ã“ã¨ã§ç¢ºå®Ÿæ€§ã¨æ­£ç¢ºæ€§ã‚’å‘ä¸Š
"""

import os
import json
import logging
import base64
from typing import Dict, List, Optional, Any
import anthropic

from global_config import *

logger = logging.getLogger(__name__)


class StepwiseAnalyzer:
    """æ®µéšçš„åˆ†æã‚¯ãƒ©ã‚¹ - JSONã‚’å±€æ‰€çš„ãƒ»æŒç¶šçš„ã«ç”Ÿæˆ"""
    
    def __init__(self, anthropic_client: anthropic.Anthropic):
        """åˆæœŸåŒ–"""
        self.anthropic_client = anthropic_client
        self.intermediate_results = {}
        
        logger.info("âœ… StepwiseAnalyzeråˆæœŸåŒ–å®Œäº†")
    
    def analyze_evidence_stepwise(self,
                                   evidence_id: str,
                                   image_paths: List[str],
                                   pdf_text: str = "") -> Dict:
        """
        è¨¼æ‹ ã‚’æ®µéšçš„ã«åˆ†æã—ã¦JSONã‚’å±€æ‰€çš„ã«ç”Ÿæˆ
        
        ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º (è¨¼æ‹ ã®åŸºæœ¬æƒ…å ±)
        ã‚¹ãƒ†ãƒƒãƒ—2: OCRãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º (å…¨ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆ)
        ã‚¹ãƒ†ãƒƒãƒ—3: æ–‡æ›¸å†…å®¹åˆ†æ (æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã€å½“äº‹è€…ãªã©)
        ã‚¹ãƒ†ãƒƒãƒ—4: æ³•çš„æ„ç¾©æŠ½å‡º (æ³•çš„è©•ä¾¡)
        ã‚¹ãƒ†ãƒƒãƒ—5: é–¢é€£äº‹å®ŸæŠ½å‡º (äº‹å®Ÿé–¢ä¿‚)
        ã‚¹ãƒ†ãƒƒãƒ—6: æœ€çµ‚çµ±åˆ (å…¨ä½“ã‚’ã¾ã¨ã‚ã‚‹)
        
        å„ã‚¹ãƒ†ãƒƒãƒ—ã§JSONã®ä¸€éƒ¨ã‚’ç”Ÿæˆã—ã€æœ€å¾Œã«çµ±åˆ
        """
        logger.info(f"ğŸ¯ æ®µéšçš„åˆ†æé–‹å§‹: {evidence_id} ({len(image_paths)}ãƒšãƒ¼ã‚¸)")
        
        # ä¸­é–“çµæœã‚’ã‚¯ãƒªã‚¢
        self.intermediate_results = {
            'evidence_id': evidence_id,
            'page_count': len(image_paths),
            'pdf_text_preview': pdf_text[:200] if pdf_text else ""
        }
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        logger.info("ğŸ“Š [1/6] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")
        metadata_json = self._step1_extract_metadata(image_paths[0], evidence_id)
        self.intermediate_results['step1_metadata'] = metadata_json
        logger.info(f"   âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {len(json.dumps(metadata_json))}æ–‡å­—")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: OCRãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆå…¨ãƒšãƒ¼ã‚¸ï¼‰
        logger.info("ğŸ“„ [2/6] OCRãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º")
        ocr_json = self._step2_extract_ocr_text(image_paths)
        self.intermediate_results['step2_ocr'] = ocr_json
        total_ocr_chars = sum(len(page.get('text', '')) for page in ocr_json.get('pages', []))
        logger.info(f"   âœ… OCRãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: {total_ocr_chars}æ–‡å­—ï¼ˆ{len(ocr_json.get('pages', []))}ãƒšãƒ¼ã‚¸ï¼‰")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: æ–‡æ›¸å†…å®¹åˆ†æ
        logger.info("ğŸ“‹ [3/6] æ–‡æ›¸å†…å®¹åˆ†æ")
        content_json = self._step3_analyze_content(image_paths, ocr_json, pdf_text)
        self.intermediate_results['step3_content'] = content_json
        logger.info(f"   âœ… æ–‡æ›¸å†…å®¹åˆ†æå®Œäº†: æ–‡æ›¸ã‚¿ã‚¤ãƒ—={content_json.get('document_type')}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: æ³•çš„æ„ç¾©æŠ½å‡º
        logger.info("âš–ï¸ [4/6] æ³•çš„æ„ç¾©æŠ½å‡º")
        legal_json = self._step4_extract_legal_significance(content_json, ocr_json)
        self.intermediate_results['step4_legal'] = legal_json
        logger.info(f"   âœ… æ³•çš„æ„ç¾©æŠ½å‡ºå®Œäº†")
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: é–¢é€£äº‹å®ŸæŠ½å‡º
        logger.info("ğŸ” [5/6] é–¢é€£äº‹å®ŸæŠ½å‡º")
        facts_json = self._step5_extract_related_facts(content_json, ocr_json, legal_json)
        self.intermediate_results['step5_facts'] = facts_json
        logger.info(f"   âœ… é–¢é€£äº‹å®ŸæŠ½å‡ºå®Œäº†")
        
        # ã‚¹ãƒ†ãƒƒãƒ—6: æœ€çµ‚çµ±åˆ
        logger.info("ğŸ”— [6/6] æœ€çµ‚çµ±åˆ")
        final_json = self._step6_final_integration(
            metadata_json, ocr_json, content_json, legal_json, facts_json
        )
        self.intermediate_results['step6_final'] = final_json
        logger.info(f"   âœ… æœ€çµ‚çµ±åˆå®Œäº†")
        
        logger.info(f"ğŸ‰ æ®µéšçš„åˆ†æå®Œäº†: {evidence_id}")
        
        return final_json
    
    def _step1_extract_metadata(self, first_image_path: str, evidence_id: str) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        - æ–‡æ›¸ã®åŸºæœ¬æƒ…å ±ã®ã¿ã‚’æŠ½å‡º
        - JSONã¯å°ã•ãå±€æ‰€çš„ã«ç”Ÿæˆ
        """
        prompt = f"""
Analyze the FIRST PAGE of this evidence document and extract ONLY basic metadata.

TASK: Extract the following information as JSON:
{{
  "evidence_id": "{evidence_id}",
  "document_basic_info": "Brief description in Japanese (1 sentence, max 50 chars)",
  "file_info": "File type and page count",
  "page_count": <number>
}}

IMPORTANT:
- Keep the JSON small and focused
- Only extract what you can see on the FIRST PAGE
- Use Japanese for descriptions
- Do NOT include OCR text yet (that comes in Step 2)
- Do NOT analyze content deeply (that comes in Step 3)

Return ONLY the JSON, no other text.
"""
        
        result = self._call_claude_vision(first_image_path, prompt, max_tokens=500)
        return self._parse_json_from_response(result)
    
    def _step2_extract_ocr_text(self, image_paths: List[str]) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—2: OCRãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆå…¨ãƒšãƒ¼ã‚¸ï¼‰
        - å„ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡º
        - åˆ†æã¯ã›ãšã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
        """
        pages_ocr = []
        
        for i, image_path in enumerate(image_paths, 1):
            logger.info(f"   ğŸ“„ ãƒšãƒ¼ã‚¸{i}ã®OCRå®Ÿè¡Œä¸­...")
            
            prompt = f"""
Perform high-accuracy OCR text extraction from this page.

TASK: Extract ALL text precisely, return as JSON:
{{
  "page": {i},
  "text": "All text from this page, character-by-character, in original language",
  "char_count": <number of characters>
}}

CRITICAL OCR REQUIREMENTS:
âœ… Extract EVERY character visible on the page
âœ… Preserve ALL formatting: line breaks, spacing, indentation
âœ… Include ALL punctuation, symbols, and special characters
âœ… For Japanese text:
  - Preserve kanji, hiragana, katakana exactly as shown
  - Include postal codes (ã€’xxx-xxxx format)
  - Include phone numbers, dates, addresses
  - Maintain vertical/horizontal text layout distinctions
âœ… For stamps/seals: Include any readable text within stamps
âœ… For headers/footers: Include all header and footer text
âœ… For tables: Preserve table structure with appropriate spacing

âŒ Do NOT translate any text
âŒ Do NOT summarize or paraphrase
âŒ Do NOT skip any portion of the text
âŒ Do NOT add interpretations or analysis

QUALITY CHECK: Ensure char_count matches actual extracted characters.

Return ONLY the JSON, no other text.
"""
            
            result = self._call_claude_vision(image_path, prompt, max_tokens=4096)
            page_json = self._parse_json_from_response(result)
            pages_ocr.append(page_json)
            
            logger.info(f"   âœ… ãƒšãƒ¼ã‚¸{i}å®Œäº†: {page_json.get('char_count', 0)}æ–‡å­—")
        
        # å…¨ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±åˆ
        full_text = '\n\n=== ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Š ===\n\n'.join(
            page.get('text', '') for page in pages_ocr
        )
        
        return {
            "pages": pages_ocr,
            "full_text": full_text,
            "total_chars": sum(page.get('char_count', 0) for page in pages_ocr)
        }
    
    def _step3_analyze_content(self, image_paths: List[str], ocr_json: Dict, pdf_text: str) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—3: æ–‡æ›¸å†…å®¹åˆ†æ
        - OCRãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ã£ã¦æ–‡æ›¸ã®å†…å®¹ã‚’åˆ†æ
        - æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã€å½“äº‹è€…ã€æ—¥ä»˜ãªã©
        """
        # OCRãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        full_ocr_text = ocr_json.get('full_text', '')
        
        prompt = f"""
Analyze this legal document based on the extracted OCR text.

OCR TEXT:
{full_ocr_text[:3000]}
{"..." if len(full_ocr_text) > 3000 else ""}

TASK: Analyze and return as JSON:
{{
  "document_type": "Document type in Japanese (é…é”è¨¼æ˜, é€šçŸ¥æ›¸, å¥‘ç´„æ›¸, etc.)",
  "sender": {{
    "name": "Sender name",
    "address": "Sender address",
    "organization": "Organization if applicable"
  }},
  "recipient": {{
    "name": "Recipient name",
    "address": "Recipient address"
  }},
  "date": "Document date if found (YYYY-MM-DD or original format)",
  "subject": "Main subject or purpose in Japanese",
  "key_entities": ["List", "of", "important", "names", "organizations"]
}}

IMPORTANT:
- Base analysis ONLY on the OCR text provided
- Use Japanese for descriptions
- Extract factual information only
- If information is not found, use null or empty string

Return ONLY the JSON, no other text.
"""
        
        # 1ãƒšãƒ¼ã‚¸ç›®ã®ç”»åƒã‚‚å‚ç…§ï¼ˆè¦–è¦šçš„ç¢ºèªç”¨ï¼‰
        result = self._call_claude_vision(image_paths[0], prompt, max_tokens=1500)
        return self._parse_json_from_response(result)
    
    def _step4_extract_legal_significance(self, content_json: Dict, ocr_json: Dict) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—4: æ³•çš„æ„ç¾©æŠ½å‡º
        - æ–‡æ›¸ã®æ³•çš„ãªæ„å‘³ã‚’åˆ†æ
        - è¨¼æ‹ èƒ½åŠ›ã€è¨¼æ˜äº‹é …ãªã©
        """
        doc_type = content_json.get('document_type', '')
        subject = content_json.get('subject', '')
        
        prompt = f"""
Analyze the legal significance of this document.

DOCUMENT INFO:
- Type: {doc_type}
- Subject: {subject}
- Sender: {content_json.get('sender', {}).get('name', '')}
- Recipient: {content_json.get('recipient', {}).get('name', '')}

TASK: Extract legal significance as JSON:
{{
  "legal_document_type": "Legal classification in Japanese",
  "evidential_value": "What this document can prove",
  "legal_implications": "Legal implications in Japanese",
  "proof_points": ["Point 1", "Point 2", "Point 3"]
}}

IMPORTANT:
- Focus on legal significance only
- Use Japanese for descriptions
- Be specific about what can be legally proven

Return ONLY the JSON, no other text.
"""
        
        result = self._call_claude_text(prompt, max_tokens=1500)
        return self._parse_json_from_response(result)
    
    def _step5_extract_related_facts(self, content_json: Dict, ocr_json: Dict, legal_json: Dict) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—5: é–¢é€£äº‹å®ŸæŠ½å‡º
        - æ–‡æ›¸ã«å«ã¾ã‚Œã‚‹äº‹å®Ÿé–¢ä¿‚ã‚’æŠ½å‡º
        """
        full_text = ocr_json.get('full_text', '')
        
        prompt = f"""
Extract factual information from this legal document.

OCR TEXT (excerpt):
{full_text[:2000]}

DOCUMENT CONTEXT:
- Type: {content_json.get('document_type', '')}
- Parties: {content_json.get('sender', {}).get('name', '')} â†’ {content_json.get('recipient', {}).get('name', '')}

TASK: Extract facts as JSON:
{{
  "chronology": [
    {{"date": "YYYY-MM-DD", "event": "What happened in Japanese"}}
  ],
  "amounts": [
    {{"type": "é‡‘é¡ç¨®é¡", "amount": "é‡‘é¡", "currency": "JPY"}}
  ],
  "claims": [
    "Claim or assertion in Japanese"
  ],
  "supporting_facts": [
    "Supporting fact in Japanese"
  ]
}}

IMPORTANT:
- Extract factual information only
- Use Japanese for descriptions
- Include dates if found
- Be specific and concrete

Return ONLY the JSON, no other text.
"""
        
        result = self._call_claude_text(prompt, max_tokens=2000)
        return self._parse_json_from_response(result)
    
    def _step6_final_integration(self,
                                  metadata_json: Dict,
                                  ocr_json: Dict,
                                  content_json: Dict,
                                  legal_json: Dict,
                                  facts_json: Dict) -> Dict:
        """
        ã‚¹ãƒ†ãƒƒãƒ—6: æœ€çµ‚çµ±åˆ
        - å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’çµ±åˆã—ã¦æœ€çµ‚JSONã‚’ç”Ÿæˆ
        """
        # è¨¼æ‹ ã®èª¬æ˜ã‚’ç”Ÿæˆ
        description = self._generate_description(content_json, ocr_json)
        
        # å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        completeness = self._calculate_completeness(
            metadata_json, ocr_json, content_json, legal_json, facts_json
        )
        
        # æœ€çµ‚JSONã‚’æ§‹ç¯‰
        final_json = {
            "evidence_id": metadata_json.get('evidence_id', ''),
            "verbalization_level": 4,
            "confidence_score": completeness,
            
            "evidence_metadata": {
                "è¨¼æ‹ ã®åŸºæœ¬æƒ…å ±": metadata_json.get('document_basic_info', ''),
                "ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±": metadata_json.get('file_info', ''),
                "ãƒšãƒ¼ã‚¸æ•°": metadata_json.get('page_count', 0)
            },
            
            "full_content": {
                "OCRãƒ†ã‚­ã‚¹ãƒˆ": ocr_json.get('full_text', ''),
                "ç·æ–‡å­—æ•°": ocr_json.get('total_chars', 0),
                "ãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆ": ocr_json.get('pages', [])
            },
            
            "è¨¼æ‹ ã®èª¬æ˜": description,
            
            "æ–‡æ›¸ã®å†…å®¹": {
                "æ–‡æ›¸ç¨®åˆ¥": content_json.get('document_type', ''),
                "å·®å‡ºäºº": content_json.get('sender', {}),
                "å—å–äºº": content_json.get('recipient', {}),
                "æ—¥ä»˜": content_json.get('date', ''),
                "ä»¶å": content_json.get('subject', ''),
                "é‡è¦ãªå›ºæœ‰åè©": content_json.get('key_entities', [])
            },
            
            "legal_significance": {
                "æ³•çš„æ–‡æ›¸ç¨®åˆ¥": legal_json.get('legal_document_type', ''),
                "è¨¼æ‹ èƒ½åŠ›": legal_json.get('evidential_value', ''),
                "æ³•çš„å«æ„": legal_json.get('legal_implications', ''),
                "è¨¼æ˜äº‹é …": legal_json.get('proof_points', [])
            },
            
            "related_facts": {
                "æ™‚ç³»åˆ—": facts_json.get('chronology', []),
                "é‡‘é¡æƒ…å ±": facts_json.get('amounts', []),
                "ä¸»å¼µå†…å®¹": facts_json.get('claims', []),
                "è£ä»˜ã‘äº‹å®Ÿ": facts_json.get('supporting_facts', [])
            },
            
            "usage_suggestions": {
                "æå‡ºã‚¿ã‚¤ãƒŸãƒ³ã‚°": self._suggest_timing(content_json, legal_json),
                "ä»–ã®è¨¼æ‹ ã¨ã®é–¢é€£": self._suggest_relations(content_json, facts_json),
                "æ³¨æ„ç‚¹": self._suggest_notes(content_json, legal_json)
            },
            
            "å®Œå…¨æ€§ã‚¹ã‚³ã‚¢": completeness
        }
        
        return final_json
    
    def _call_claude_vision(self, image_path: str, prompt: str, max_tokens: int = 2048) -> str:
        """Claude Vision APIã‚’å‘¼ã³å‡ºã—"""
        with open(image_path, 'rb') as f:
            image_data = base64.b64encode(f.read()).decode('utf-8')
        
        mime_type = 'image/jpeg'
        if image_path.endswith('.png'):
            mime_type = 'image/png'
        
        try:
            response = self.anthropic_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=max_tokens,
                messages=[{
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": mime_type,
                                "data": image_data
                            }
                        },
                        {
                            "type": "text",
                            "text": prompt
                        }
                    ]
                }]
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"Claude Vision APIå‘¼ã³å‡ºã—å¤±æ•—: {e}")
            return "{}"
    
    def _call_claude_text(self, prompt: str, max_tokens: int = 2048) -> str:
        """Claude Text APIã‚’å‘¼ã³å‡ºã—"""
        try:
            response = self.anthropic_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=max_tokens,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"Claude Text APIå‘¼ã³å‡ºã—å¤±æ•—: {e}")
            return "{}"
    
    def _parse_json_from_response(self, response: str) -> Dict:
        """APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰JSONã‚’æŠ½å‡ºã—ã¦ãƒ‘ãƒ¼ã‚¹"""
        try:
            # ```json ``` ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
            if '```json' in response:
                json_str = response.split('```json')[1].split('```')[0].strip()
            elif '```' in response:
                json_str = response.split('```')[1].split('```')[0].strip()
            else:
                json_str = response.strip()
            
            return json.loads(json_str)
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
            logger.debug(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response[:500]}")
            return {}
        except Exception as e:
            logger.error(f"JSONæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _generate_description(self, content_json: Dict, ocr_json: Dict) -> str:
        """è¨¼æ‹ ã®èª¬æ˜ã‚’ç”Ÿæˆ"""
        doc_type = content_json.get('document_type', 'æ–‡æ›¸')
        sender = content_json.get('sender', {}).get('name', 'ä¸æ˜')
        recipient = content_json.get('recipient', {}).get('name', 'ä¸æ˜')
        subject = content_json.get('subject', '')
        
        description = f"{doc_type}ã€‚{sender}ã‹ã‚‰{recipient}ã¸ã®æ–‡æ›¸ã€‚"
        if subject:
            description += f"ä»¶åï¼š{subject}ã€‚"
        
        return description
    
    def _calculate_completeness(self, *args) -> float:
        """å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœãŒæƒã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        total_steps = len(args)
        completed_steps = sum(1 for arg in args if arg and isinstance(arg, dict) and len(arg) > 0)
        
        return round(completed_steps / total_steps, 2)
    
    def _suggest_timing(self, content_json: Dict, legal_json: Dict) -> str:
        """æå‡ºã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®ææ¡ˆ"""
        return "äº‹å®Ÿé–¢ä¿‚ã®ä¸»å¼µæ™‚ã€ã¾ãŸã¯ç›¸æ‰‹æ–¹ã®ä¸»å¼µã¸ã®åè«–æ™‚ã«ä½¿ç”¨å¯èƒ½"
    
    def _suggest_relations(self, content_json: Dict, facts_json: Dict) -> str:
        """ä»–ã®è¨¼æ‹ ã¨ã®é–¢é€£ã®ææ¡ˆ"""
        return "æ™‚ç³»åˆ—ã§å‰å¾Œã™ã‚‹è¨¼æ‹ ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€äº‹å®Ÿã®é€£ç¶šæ€§ã‚’ç¤ºã›ã‚‹"
    
    def _suggest_notes(self, content_json: Dict, legal_json: Dict) -> str:
        """æ³¨æ„ç‚¹ã®ææ¡ˆ"""
        doc_type = content_json.get('document_type', '')
        if 'é…é”è¨¼æ˜' in doc_type:
            return "é…é”è¨¼æ˜æ›¸ã¯é€é”ã®äº‹å®Ÿã‚’è¨¼æ˜ã™ã‚‹ãŒã€æ–‡æ›¸å†…å®¹ã®çœŸå®Ÿæ€§ã¯åˆ¥é€”ç«‹è¨¼ãŒå¿…è¦"
        else:
            return "æ–‡æ›¸ã®çœŸæ­£æ€§ï¼ˆä½œæˆè€…ãŒæœ¬å½“ã«ä½œæˆã—ãŸã‹ï¼‰ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚‹"
