#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼çµ„ã¿ç«‹ã¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (æ‹¡å¼µç‰ˆ)

ã€æ©Ÿèƒ½ã€‘
- è¨¼æ‹ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰è¨¼æ‹ ã‚’æŠ½å‡º
- è¨¼æ‹ ã®ä½œæˆå¹´æœˆæ—¥ã‚„é–¢é€£ã™ã‚‹æ™‚é–“æƒ…å ±ã‚’è§£æ
- æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆã—ã¦å®¢è¦³çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç”Ÿæˆ
- æ³•çš„åˆ¤æ–­ã‚’å«ã¾ãªã„äº‹å®Ÿã®æµã‚Œã‚’æ§‹ç¯‰
- AI ã«ã‚ˆã‚‹å®¢è¦³çš„ãƒŠãƒ©ãƒ†ã‚£ãƒ–ç”Ÿæˆ
- è¨¼æ‹ é–“ã®é–¢é€£æ€§åˆ†æ
- å¤šæ§˜ãªå‡ºåŠ›å½¢å¼å¯¾å¿œ (JSON, Markdown, HTML, ãƒ†ã‚­ã‚¹ãƒˆ)

ã€ä½¿ç”¨æ–¹æ³•ã€‘
    from timeline_builder import TimelineBuilder
    
    builder = TimelineBuilder(case_manager, current_case)
    timeline = builder.build_timeline()
    
    # å®¢è¦³çš„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç”Ÿæˆ
    narrative = builder.generate_objective_narrative(timeline)
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    builder.export_timeline(timeline, output_format="json")
    builder.export_timeline(timeline, output_format="markdown")
    builder.export_timeline(timeline, output_format="html")
"""

import os
import sys
import json
import re
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
from collections import defaultdict
from pathlib import Path

try:
    import global_config as gconfig
    from src.case_manager import CaseManager
    from src.gdrive_database_manager import GDriveDatabaseManager, create_database_manager
    from anthropic import Anthropic
    from dotenv import load_dotenv
    from googleapiclient.http import MediaFileUpload
    
    # ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
    load_dotenv()
except ImportError as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    sys.exit(1)


class TimelineEvent:
    """æ™‚ç³»åˆ—ã‚¤ãƒ™ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
    
    def __init__(self, date: Optional[str], evidence_id: str, evidence_number: str,
                 description: str, confidence: str = "ç¢ºå®Ÿ",
                 source_type: str = "evidence",
                 related_facts: Optional[List[Dict]] = None,
                 legal_significance: Optional[Dict] = None,
                 temporal_context: Optional[str] = None):
        """
        Args:
            date: ã‚¤ãƒ™ãƒ³ãƒˆæ—¥æ™‚ï¼ˆYYYY-MM-DDå½¢å¼ã€ã¾ãŸã¯YYYY-MMã€YYYYï¼‰
            evidence_id: è¨¼æ‹ IDï¼ˆä¾‹: ko070ï¼‰ã¾ãŸã¯ä¾é ¼è€…ç™ºè¨€IDï¼ˆä¾‹: client_001ï¼‰
            evidence_number: è¨¼æ‹ ç•ªå·ï¼ˆä¾‹: ç”²070ï¼‰ã¾ãŸã¯ã€Œä¾é ¼è€…ç™ºè¨€ã€
            description: ã‚¤ãƒ™ãƒ³ãƒˆã®èª¬æ˜
            confidence: æ—¥ä»˜ã®ç¢ºå®Ÿæ€§ï¼ˆ"ç¢ºå®Ÿ", "æ¨å®š", "ä¸æ˜"ï¼‰
            source_type: æƒ…å ±æºã®ç¨®é¡ï¼ˆ"evidence": è¨¼æ‹ , "client_statement": ä¾é ¼è€…ç™ºè¨€ï¼‰
            related_facts: é–¢é€£ã™ã‚‹äº‹å®Ÿã®ãƒªã‚¹ãƒˆï¼ˆdatabase.jsonã®ai_analysis.related_factsï¼‰
            legal_significance: æ³•çš„é‡è¦æ€§ï¼ˆdatabase.jsonã®ai_analysis.legal_significanceï¼‰
            temporal_context: æ™‚ç³»åˆ—çš„ãªæ–‡è„ˆï¼ˆdatabase.jsonã®temporal_information.timelineï¼‰
        """
        self.date = date
        self.evidence_id = evidence_id
        self.evidence_number = evidence_number
        self.description = description
        self.confidence = confidence
        self.source_type = source_type
        self.related_facts = related_facts or []
        self.legal_significance = legal_significance or {}
        self.temporal_context = temporal_context
        
        # æ—¥ä»˜ã®ç²’åº¦ã‚’åˆ¤å®š
        self.date_precision = self._determine_date_precision()
        
        # ã‚½ãƒ¼ãƒˆç”¨ã®ã‚­ãƒ¼ç”Ÿæˆ
        self.sort_key = self._generate_sort_key()
    
    def _determine_date_precision(self) -> str:
        """æ—¥ä»˜ã®ç²’åº¦ã‚’åˆ¤å®š"""
        if not self.date:
            return "unknown"
        
        if re.match(r'^\d{4}-\d{2}-\d{2}$', self.date):
            return "day"
        elif re.match(r'^\d{4}-\d{2}$', self.date):
            return "month"
        elif re.match(r'^\d{4}$', self.date):
            return "year"
        else:
            return "unknown"
    
    def _generate_sort_key(self) -> Tuple:
        """ã‚½ãƒ¼ãƒˆç”¨ã®ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        if not self.date:
            # æ—¥ä»˜ä¸æ˜ã¯æœ€å¾Œ
            return (9999, 99, 99, self.evidence_id)
        
        # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã‚½ãƒ¼ãƒˆç”¨ã®ã‚¿ãƒ—ãƒ«ã‚’ç”Ÿæˆ
        parts = self.date.split('-')
        year = int(parts[0]) if len(parts) > 0 else 9999
        month = int(parts[1]) if len(parts) > 1 else 99
        day = int(parts[2]) if len(parts) > 2 else 99
        
        return (year, month, day, self.evidence_id)
    
    def format_date_display(self) -> str:
        """è¡¨ç¤ºç”¨ã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        if not self.date:
            return "æ—¥ä»˜ä¸æ˜"
        
        if self.date_precision == "day":
            return self.date
        elif self.date_precision == "month":
            return f"{self.date}é ƒ"
        elif self.date_precision == "year":
            return f"{self.date}å¹´"
        else:
            return self.date
    
    def to_dict(self) -> Dict:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "date": self.date,
            "date_display": self.format_date_display(),
            "date_precision": self.date_precision,
            "confidence": self.confidence,
            "evidence_id": self.evidence_id,
            "evidence_number": self.evidence_number,
            "description": self.description,
            "source_type": self.source_type,
            "related_facts": self.related_facts,
            "legal_significance": self.legal_significance,
            "temporal_context": self.temporal_context
        }


class TimelineBuilder:
    """æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼çµ„ã¿ç«‹ã¦ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, case_manager: CaseManager, current_case: Dict, use_ai: bool = True):
        """åˆæœŸåŒ–
        
        Args:
            case_manager: CaseManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            current_case: ç¾åœ¨ã®äº‹ä»¶æƒ…å ±
            use_ai: AI ã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        """
        self.case_manager = case_manager
        self.current_case = current_case
        self.use_ai = use_ai
        
        # Google Drive Database Managerã‚’åˆæœŸåŒ–
        self.db_manager = create_database_manager(case_manager, current_case)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰
        self.database = self._load_database()
        
        # Anthropic Claude ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        if self.use_ai:
            api_key = os.getenv('ANTHROPIC_API_KEY')
            if api_key:
                self.anthropic_client = Anthropic(api_key=api_key)
            else:
                print("âš ï¸ ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AI æ©Ÿèƒ½ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚")
                print("   .env ãƒ•ã‚¡ã‚¤ãƒ«ã« ANTHROPIC_API_KEY=sk-ant-... ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
                self.use_ai = False
                self.anthropic_client = None
        else:
            self.anthropic_client = None
    
    def _load_database(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            db_data = self.db_manager.load_database()
            if not db_data:
                print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ç©ºã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                return {
                    "metadata": {},
                    "case_info": {},
                    "evidence": [],
                    "phase1_progress": []
                }
            return db_data
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return {
                "metadata": {},
                "case_info": {},
                "evidence": [],
                "phase1_progress": []
            }
    
    def _load_client_statements(self) -> List[Dict]:
        """ä¾é ¼è€…ç™ºè¨€ç­‰ã‚’èª­ã¿è¾¼ã‚€ï¼ˆæ—¥ä»˜æŒ‡å®šã‚ã‚Šï¼‰
        
        Returns:
            ä¾é ¼è€…ç™ºè¨€ã®ãƒªã‚¹ãƒˆ
        """
        try:
            case_id = self.current_case.get('case_id', 'unknown')
            statements_path = os.path.join(gconfig.LOCAL_WORK_DIR, case_id, 'client_statements.json')
            
            if not os.path.exists(statements_path):
                return []
            
            with open(statements_path, 'r', encoding='utf-8') as f:
                statements_data = json.load(f)
                return statements_data.get('statements', [])
                
        except Exception as e:
            print(f"âš ï¸ ä¾é ¼è€…ç™ºè¨€ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return []
    
    def _load_general_context(self) -> List[Dict]:
        """åŒ…æ‹¬çš„ãªä¾é ¼è€…ç™ºè¨€ãƒ»ãƒ¡ãƒ¢ã‚’èª­ã¿è¾¼ã‚€ï¼ˆæ—¥ä»˜ãªã—ã€è¤‡æ•°äº‹å®Ÿã«ã‚ãŸã‚‹ï¼‰
        
        Returns:
            åŒ…æ‹¬çš„ç™ºè¨€ã®ãƒªã‚¹ãƒˆ
        """
        try:
            case_id = self.current_case.get('case_id', 'unknown')
            context_path = os.path.join(gconfig.LOCAL_WORK_DIR, case_id, 'client_general_context.json')
            
            if not os.path.exists(context_path):
                return []
            
            with open(context_path, 'r', encoding='utf-8') as f:
                context_data = json.load(f)
                return context_data.get('contexts', [])
                
        except Exception as e:
            print(f"âš ï¸ åŒ…æ‹¬çš„ç™ºè¨€ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return []
    
    def _save_general_context(self, contexts: List[Dict]) -> bool:
        """åŒ…æ‹¬çš„ãªä¾é ¼è€…ç™ºè¨€ãƒ»ãƒ¡ãƒ¢ã‚’ä¿å­˜
        
        Args:
            contexts: åŒ…æ‹¬çš„ç™ºè¨€ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            case_id = self.current_case.get('case_id', 'unknown')
            case_dir = os.path.join(gconfig.LOCAL_WORK_DIR, case_id)
            os.makedirs(case_dir, exist_ok=True)
            
            context_path = os.path.join(case_dir, 'client_general_context.json')
            
            context_data = {
                "metadata": {
                    "case_id": case_id,
                    "case_name": self.current_case.get('case_name', 'ä¸æ˜'),
                    "last_updated": datetime.now().isoformat(),
                    "description": "ä¾é ¼è€…ã®åŒ…æ‹¬çš„ãªç™ºè¨€ãƒ»ãƒ¡ãƒ¢ï¼ˆæ—¥ä»˜ãªã—ã€è¤‡æ•°äº‹å®Ÿã«ã‚ãŸã‚‹å…¨ä½“çš„ãªæ–‡è„ˆæƒ…å ±ï¼‰"
                },
                "contexts": contexts
            }
            
            with open(context_path, 'w', encoding='utf-8') as f:
                json.dump(context_data, f, ensure_ascii=False, indent=2)
            
            return True
            
        except Exception as e:
            print(f"âŒ åŒ…æ‹¬çš„ç™ºè¨€ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False
    
    def _save_client_statements(self, statements: List[Dict]) -> bool:
        """ä¾é ¼è€…ç™ºè¨€ç­‰ã‚’ä¿å­˜
        
        Args:
            statements: ä¾é ¼è€…ç™ºè¨€ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            case_id = self.current_case.get('case_id', 'unknown')
            case_dir = os.path.join(gconfig.LOCAL_WORK_DIR, case_id)
            os.makedirs(case_dir, exist_ok=True)
            
            statements_path = os.path.join(case_dir, 'client_statements.json')
            
            statements_data = {
                "metadata": {
                    "case_id": case_id,
                    "case_name": self.current_case.get('case_name', 'ä¸æ˜'),
                    "last_updated": datetime.now().isoformat()
                },
                "statements": statements
            }
            
            with open(statements_path, 'w', encoding='utf-8') as f:
                json.dump(statements_data, f, ensure_ascii=False, indent=2)
            
            return True
            
        except Exception as e:
            print(f"âŒ ä¾é ¼è€…ç™ºè¨€ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False
    
    def extract_date_from_evidence(self, evidence: Dict) -> Optional[str]:
        """è¨¼æ‹ ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
        
        å„ªå…ˆé †ä½:
        1. ai_analysis.objective_analysis.temporal_information.document_date (CSVç·¨é›†ã§æ›´æ–°ã•ã‚Œã‚‹)
        2. complete_metadata.format_specific.document_date
        3. complete_metadata.format_specific.exif_data.DateTime*
        4. complete_metadata.basic.created_time
        5. ai_analysis.evidence_metadata.creation_date
        6. ai_analysis.related_facts.timeline ã®æœ€åˆã®æ—¥ä»˜
        
        Returns:
            æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DD, YYYY-MM, YYYYã®ã„ãšã‚Œã‹ï¼‰ã¾ãŸã¯None
        """
        try:
            # Phase1åˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—
            phase1_analysis = evidence.get('phase1_complete_analysis', {})
            ai_analysis = phase1_analysis.get('ai_analysis', {})
            
            # 1. AIåˆ†æã®objective_analysis.temporal_informationã‹ã‚‰å–å¾—ï¼ˆæœ€å„ªå…ˆ: CSVç·¨é›†ã§æ›´æ–°ï¼‰
            objective_analysis = ai_analysis.get('objective_analysis', {})
            temporal_info = objective_analysis.get('temporal_information', {})
            if 'document_date' in temporal_info:
                date_str = temporal_info['document_date']
                parsed_date = self._parse_date(date_str)
                if parsed_date:
                    return parsed_date
            
            # 2. complete_metadataã‹ã‚‰æ–‡æ›¸æ—¥ä»˜ã‚’å–å¾—
            complete_metadata = evidence.get('complete_metadata', {})
            format_specific = complete_metadata.get('format_specific', {})
            
            if 'document_date' in format_specific:
                date_str = format_specific['document_date']
                parsed_date = self._parse_date(date_str)
                if parsed_date:
                    return parsed_date
            
            # 3. EXIFæƒ…å ±ã‹ã‚‰å–å¾—
            exif_data = format_specific.get('exif_data', {})
            for key in ['DateTime', 'DateTimeOriginal', 'DateTimeDigitized']:
                if key in exif_data:
                    date_str = exif_data[key]
                    parsed_date = self._parse_date(date_str)
                    if parsed_date:
                        return parsed_date
            
            # 4. ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæ—¥æ™‚ã‹ã‚‰å–å¾—
            basic_metadata = complete_metadata.get('basic', {})
            if 'created_time' in basic_metadata:
                date_str = basic_metadata['created_time']
                parsed_date = self._parse_date(date_str)
                if parsed_date:
                    return parsed_date
            
            # 5. AIåˆ†æã®evidence_metadataã‹ã‚‰å–å¾—
            evidence_metadata = ai_analysis.get('evidence_metadata', {})
            if 'creation_date' in evidence_metadata:
                date_str = evidence_metadata['creation_date']
                parsed_date = self._parse_date(date_str)
                if parsed_date:
                    return parsed_date
            
            # 6. related_factsã®timelineã‹ã‚‰å–å¾—
            related_facts = ai_analysis.get('related_facts', {})
            timeline = related_facts.get('timeline', [])
            if timeline and len(timeline) > 0:
                # æœ€åˆã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
                first_event = timeline[0]
                date_str = self._extract_date_from_text(first_event)
                if date_str:
                    return date_str
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ è¨¼æ‹  {evidence.get('evidence_id', 'ä¸æ˜')} ã®æ—¥ä»˜æŠ½å‡ºã«å¤±æ•—: {e}")
            return None
    
    def _parse_date(self, date_str: str) -> Optional[str]:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ¨™æº–å½¢å¼ã«å¤‰æ›
        
        Returns:
            YYYY-MM-DD, YYYY-MM, YYYY ã®ã„ãšã‚Œã‹ã®å½¢å¼ã€ã¾ãŸã¯None
        """
        if not date_str or not isinstance(date_str, str):
            return None
        
        # æ—¢ã«æ¨™æº–å½¢å¼ã®å ´åˆ
        if re.match(r'^\d{4}-\d{2}-\d{2}$', date_str):
            return date_str
        if re.match(r'^\d{4}-\d{2}$', date_str):
            return date_str
        if re.match(r'^\d{4}$', date_str):
            return date_str
        
        # ISO 8601å½¢å¼ï¼ˆYYYY-MM-DDTHH:MM:SS...ï¼‰
        iso_match = re.match(r'^(\d{4})-(\d{2})-(\d{2})T', date_str)
        if iso_match:
            return f"{iso_match.group(1)}-{iso_match.group(2)}-{iso_match.group(3)}"
        
        # ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼ï¼ˆYYYY-MM-DD HH:MM:SS...ï¼‰
        space_timestamp_match = re.match(r'^(\d{4})-(\d{2})-(\d{2})\s', date_str)
        if space_timestamp_match:
            return f"{space_timestamp_match.group(1)}-{space_timestamp_match.group(2)}-{space_timestamp_match.group(3)}"
        
        # EXIFå½¢å¼ï¼ˆYYYY:MM:DD HH:MM:SSï¼‰
        exif_match = re.match(r'^(\d{4}):(\d{2}):(\d{2})', date_str)
        if exif_match:
            return f"{exif_match.group(1)}-{exif_match.group(2)}-{exif_match.group(3)}"
        
        # æ—¥æœ¬èªå½¢å¼ï¼ˆYYYYå¹´MMæœˆDDæ—¥ï¼‰
        ja_full_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', date_str)
        if ja_full_match:
            year = ja_full_match.group(1)
            month = ja_full_match.group(2).zfill(2)
            day = ja_full_match.group(3).zfill(2)
            return f"{year}-{month}-{day}"
        
        # æ—¥æœ¬èªå½¢å¼ï¼ˆYYYYå¹´MMæœˆï¼‰
        ja_month_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', date_str)
        if ja_month_match:
            year = ja_month_match.group(1)
            month = ja_month_match.group(2).zfill(2)
            return f"{year}-{month}"
        
        # æ—¥æœ¬èªå½¢å¼ï¼ˆYYYYå¹´ï¼‰
        ja_year_match = re.search(r'(\d{4})å¹´', date_str)
        if ja_year_match:
            return ja_year_match.group(1)
        
        # YYYY/MM/DDå½¢å¼
        slash_match = re.match(r'^(\d{4})/(\d{2})/(\d{2})', date_str)
        if slash_match:
            return f"{slash_match.group(1)}-{slash_match.group(2)}-{slash_match.group(3)}"
        
        return None
    
    def _extract_date_from_text(self, text: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
        # å¹´æœˆæ—¥å½¢å¼
        match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', text)
        if match:
            year = match.group(1)
            month = match.group(2).zfill(2)
            day = match.group(3).zfill(2)
            return f"{year}-{month}-{day}"
        
        # å¹´æœˆå½¢å¼
        match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ', text)
        if match:
            year = match.group(1)
            month = match.group(2).zfill(2)
            return f"{year}-{month}"
        
        # å¹´ã®ã¿
        match = re.search(r'(\d{4})å¹´', text)
        if match:
            return match.group(1)
        
        return None
    
    def extract_description_from_evidence(self, evidence: Dict) -> str:
        """è¨¼æ‹ ã‹ã‚‰èª¬æ˜æ–‡ã‚’æŠ½å‡ºï¼ˆæ—§æ§‹é€ ãƒ»æ–°æ§‹é€ ã®ä¸¡æ–¹ã«å¯¾å¿œï¼‰"""
        try:
            phase1_analysis = evidence.get('phase1_complete_analysis', {})
            ai_analysis = phase1_analysis.get('ai_analysis', {})
            
            # full_contentã‹ã‚‰å®Œå…¨ãªèª¬æ˜ã‚’å–å¾—
            full_content = ai_analysis.get('full_content', {})
            
            # æ—§æ§‹é€ : complete_description
            complete_description = full_content.get('complete_description', '')
            
            # æ–°æ§‹é€ : OCRãƒ†ã‚­ã‚¹ãƒˆã€è¨¼æ‹ ã®èª¬æ˜ã€æ–‡æ›¸ã®å†…å®¹ã‚’è©¦ã™
            if not complete_description:
                complete_description = full_content.get('OCRãƒ†ã‚­ã‚¹ãƒˆ', '')
            if not complete_description:
                complete_description = ai_analysis.get('è¨¼æ‹ ã®èª¬æ˜', '')
            if not complete_description:
                complete_description = ai_analysis.get('æ–‡æ›¸ã®å†…å®¹', '')
            
            if complete_description:
                # é•·ã™ãã‚‹å ´åˆã¯è¦ç´„ï¼ˆæœ€åˆã®500æ–‡å­—ï¼‰
                if len(complete_description) > 500:
                    return complete_description[:500] + "..."
                return complete_description
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: evidence_metadataã‹ã‚‰åŸºæœ¬æƒ…å ±
            evidence_metadata = ai_analysis.get('evidence_metadata', {})
            
            # æ—§æ§‹é€ 
            doc_type = evidence_metadata.get('document_type', '')
            summary = evidence_metadata.get('summary', '')
            
            # æ–°æ§‹é€ 
            if not doc_type:
                doc_type = evidence_metadata.get('è¨¼æ‹ ã®åŸºæœ¬æƒ…å ±', '')
            if not summary:
                summary = evidence_metadata.get('ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±', '')
            
            # ä½•ã‚‚å–å¾—ã§ããªã‹ã£ãŸå ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            if not doc_type:
                doc_type = 'ä¸æ˜ãªæ–‡æ›¸'
            
            if summary:
                return f"{doc_type}: {summary}"
            
            return f"{doc_type}ï¼ˆè©³ç´°æƒ…å ±ãªã—ï¼‰"
            
        except Exception as e:
            print(f"âš ï¸ è¨¼æ‹  {evidence.get('evidence_id', 'ä¸æ˜')} ã®èª¬æ˜æŠ½å‡ºã«å¤±æ•—: {e}")
            return "èª¬æ˜æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ"
    
    def _clean_temporal_references(self, text: str) -> str:
        """è¨¼æ‹ èª¬æ˜æ–‡ã‹ã‚‰æ™‚é–“çš„ãªæ··ä¹±ã‚’æ‹›ãè¡¨ç¾ã‚’é™¤å»
        
        Args:
            text: å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not text:
            return text
        
        try:
            # é™¤å»ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            temporal_patterns = [
                # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæ’®å½±æ—¥/è¨˜éŒ²æ—¥ã®è¨˜è¼‰
                r'ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæ’®å½±æ—¥[ï¼š:ï¼š]\s*\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}æ—¥?',
                r'è¨˜éŒ²æ—¥[ï¼š:ï¼š]\s*\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}æ—¥?',
                r'å–å¾—æ—¥[ï¼š:ï¼š]\s*\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}æ—¥?',
                r'æ’®å½±æ—¥[ï¼š:ï¼š]\s*\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}æ—¥?',
                r'ã‚­ãƒ£ãƒ—ãƒãƒ£æ—¥[ï¼š:ï¼š]\s*\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}æ—¥?',
                # ã€Œï½ã«è¨˜éŒ²ã•ã‚ŒãŸã€ã€Œï½ã«æ’®å½±ã•ã‚ŒãŸã€ç­‰ã®è¡¨ç¾
                r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}æ—¥?ã«è¨˜éŒ²ã•ã‚ŒãŸ?',
                r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}æ—¥?ã«æ’®å½±ã•ã‚ŒãŸ?',
                r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}æ—¥?ã«å–å¾—ã•ã‚ŒãŸ?',
                r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}æ—¥?ã«ã‚­ãƒ£ãƒ—ãƒãƒ£ã•ã‚ŒãŸ?',
            ]
            
            cleaned = text
            for pattern in temporal_patterns:
                cleaned = re.sub(pattern, '', cleaned)
            
            # ä½™åˆ†ãªç©ºç™½ã‚„æ”¹è¡Œã‚’æ•´ç†
            cleaned = re.sub(r'\n\s*\n', '\n', cleaned)  # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’1ã¤ã«
            cleaned = re.sub(r'  +', ' ', cleaned)  # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
            
            return cleaned.strip()
            
        except Exception as e:
            print(f"âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã«å¤±æ•—: {e}")
            return text
    
    def extract_related_facts_from_evidence(self, evidence: Dict) -> List[Dict]:
        """è¨¼æ‹ ã‹ã‚‰related_factsæƒ…å ±ã‚’æŠ½å‡º
        
        Returns:
            é–¢é€£ã™ã‚‹äº‹å®Ÿã®ãƒªã‚¹ãƒˆ
        """
        try:
            phase1_analysis = evidence.get('phase1_complete_analysis', {})
            ai_analysis = phase1_analysis.get('ai_analysis', {})
            related_facts = ai_analysis.get('related_facts', {})
            
            # related_factsã®å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’åé›†
            facts_list = []
            
            # key_factsã‚’è¿½åŠ 
            if 'key_facts' in related_facts and related_facts['key_facts']:
                for fact in related_facts['key_facts']:
                    facts_list.append({
                        'type': 'key_fact',
                        'content': fact
                    })
            
            # timelineã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ 
            if 'timeline' in related_facts and related_facts['timeline']:
                for timeline_entry in related_facts['timeline']:
                    facts_list.append({
                        'type': 'timeline',
                        'content': timeline_entry
                    })
            
            # contextual_backgroundã‚’è¿½åŠ 
            if 'contextual_background' in related_facts and related_facts['contextual_background']:
                facts_list.append({
                    'type': 'context',
                    'content': related_facts['contextual_background']
                })
            
            return facts_list
            
        except Exception as e:
            print(f"âš ï¸ è¨¼æ‹  {evidence.get('evidence_id', 'ä¸æ˜')} ã®related_factsæŠ½å‡ºã«å¤±æ•—: {e}")
            return []
    
    def extract_legal_significance_from_evidence(self, evidence: Dict) -> Dict:
        """è¨¼æ‹ ã‹ã‚‰legal_significanceæƒ…å ±ã‚’æŠ½å‡º
        
        Returns:
            æ³•çš„é‡è¦æ€§æƒ…å ±ã®è¾æ›¸
        """
        try:
            phase1_analysis = evidence.get('phase1_complete_analysis', {})
            ai_analysis = phase1_analysis.get('ai_analysis', {})
            legal_significance = ai_analysis.get('legal_significance', {})
            
            return {
                'relevance_assessment': legal_significance.get('relevance_assessment', ''),
                'key_legal_points': legal_significance.get('key_legal_points', []),
                'evidentiary_value': legal_significance.get('evidentiary_value', ''),
                'procedural_considerations': legal_significance.get('procedural_considerations', [])
            }
            
        except Exception as e:
            print(f"âš ï¸ è¨¼æ‹  {evidence.get('evidence_id', 'ä¸æ˜')} ã®legal_significanceæŠ½å‡ºã«å¤±æ•—: {e}")
            return {}
    
    def extract_temporal_context_from_evidence(self, evidence: Dict) -> Optional[str]:
        """è¨¼æ‹ ã‹ã‚‰temporal_information.timelineæƒ…å ±ã‚’æŠ½å‡º
        
        Returns:
            æ™‚ç³»åˆ—çš„ãªæ–‡è„ˆã®èª¬æ˜
        """
        try:
            phase1_analysis = evidence.get('phase1_complete_analysis', {})
            ai_analysis = phase1_analysis.get('ai_analysis', {})
            temporal_info = ai_analysis.get('temporal_information', {})
            
            return temporal_info.get('timeline', None)
            
        except Exception as e:
            print(f"âš ï¸ è¨¼æ‹  {evidence.get('evidence_id', 'ä¸æ˜')} ã®temporal_contextæŠ½å‡ºã«å¤±æ•—: {e}")
            return None
    
    def add_client_statement(self, date: str, statement: str, context: Optional[str] = None) -> bool:
        """ä¾é ¼è€…ç™ºè¨€ã‚’è¿½åŠ ï¼ˆæ—¥ä»˜æŒ‡å®šã‚ã‚Šï¼‰
        
        Args:
            date: ç™ºè¨€æ—¥ã¾ãŸã¯ç™ºè¨€ã«é–¢ã™ã‚‹æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
            statement: ç™ºè¨€å†…å®¹
            context: ç™ºè¨€ã®æ–‡è„ˆã‚„çŠ¶æ³èª¬æ˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            statements = self._load_client_statements()
            
            # æ–°ã—ã„statement IDã‚’ç”Ÿæˆ
            statement_id = f"client_{len(statements) + 1:03d}"
            
            new_statement = {
                "statement_id": statement_id,
                "date": date,
                "statement": statement,
                "context": context,
                "added_at": datetime.now().isoformat()
            }
            
            statements.append(new_statement)
            return self._save_client_statements(statements)
            
        except Exception as e:
            print(f"âŒ ä¾é ¼è€…ç™ºè¨€ã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False
    
    def add_general_context(self, title: str, content: str, category: Optional[str] = None) -> bool:
        """åŒ…æ‹¬çš„ãªä¾é ¼è€…ç™ºè¨€ãƒ»ãƒ¡ãƒ¢ã‚’è¿½åŠ ï¼ˆæ—¥ä»˜ãªã—ã€è¤‡æ•°äº‹å®Ÿã«ã‚ãŸã‚‹ï¼‰
        
        Args:
            title: ç™ºè¨€ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚„æ¦‚è¦
            content: ç™ºè¨€ã®è©³ç´°å†…å®¹
            category: ã‚«ãƒ†ã‚´ãƒªï¼ˆä¾‹: "äº‹ä»¶ã®èƒŒæ™¯", "äººç‰©é–¢ä¿‚", "å…¨ä½“çš„ãªçµŒç·¯"ãªã©ï¼‰
        
        Returns:
            æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            contexts = self._load_general_context()
            
            # æ–°ã—ã„context IDã‚’ç”Ÿæˆ
            context_id = f"context_{len(contexts) + 1:03d}"
            
            new_context = {
                "context_id": context_id,
                "title": title,
                "content": content,
                "category": category or "ä¸€èˆ¬",
                "added_at": datetime.now().isoformat()
            }
            
            contexts.append(new_context)
            return self._save_general_context(contexts)
            
        except Exception as e:
            print(f"âŒ åŒ…æ‹¬çš„ç™ºè¨€ã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return False
    
    def build_timeline(self, include_client_statements: bool = True) -> List[TimelineEvent]:
        """æ™‚ç³»åˆ—ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰
        
        Args:
            include_client_statements: ä¾é ¼è€…ç™ºè¨€ã‚’å«ã‚ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        
        Returns:
            æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸTimelineEventã®ãƒªã‚¹ãƒˆ
        """
        print("\n" + "="*80)
        print("æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æ§‹ç¯‰ã‚’é–‹å§‹ã—ã¾ã™...")
        print("="*80)
        
        evidence_list = self.database.get('evidence', [])
        
        if not evidence_list:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨¼æ‹ ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return []
        
        print(f"\nğŸ“Š åˆ†æå¯¾è±¡: {len(evidence_list)}ä»¶ã®è¨¼æ‹ ")
        
        timeline_events = []
        no_date_count = 0
        
        for evidence in evidence_list:
            evidence_id = evidence.get('evidence_id', 'ä¸æ˜')
            evidence_number = evidence.get('evidence_number', 'ä¸æ˜')
            
            # æ—¥ä»˜ã‚’æŠ½å‡º
            date = self.extract_date_from_evidence(evidence)
            
            # èª¬æ˜ã‚’æŠ½å‡º
            description = self.extract_description_from_evidence(evidence)
            
            # è©³ç´°æƒ…å ±ã‚’æŠ½å‡ºï¼ˆdatabase.jsonã®AIåˆ†æçµæœã‚’æ´»ç”¨ï¼‰
            related_facts = self.extract_related_facts_from_evidence(evidence)
            legal_significance = self.extract_legal_significance_from_evidence(evidence)
            temporal_context = self.extract_temporal_context_from_evidence(evidence)
            
            # æ—¥ä»˜ã®ç¢ºå®Ÿæ€§ã‚’åˆ¤å®š
            confidence = "ç¢ºå®Ÿ" if date else "ä¸æ˜"
            
            if not date:
                no_date_count += 1
            
            # TimelineEventã‚’ä½œæˆï¼ˆæ‹¡å¼µç‰ˆï¼‰
            event = TimelineEvent(
                date=date,
                evidence_id=evidence_id,
                evidence_number=evidence_number,
                description=description,
                confidence=confidence,
                source_type="evidence",
                related_facts=related_facts,
                legal_significance=legal_significance,
                temporal_context=temporal_context
            )
            
            timeline_events.append(event)
        
        # ä¾é ¼è€…ç™ºè¨€ã‚’è¿½åŠ 
        if include_client_statements:
            client_statements = self._load_client_statements()
            if client_statements:
                print(f"\nğŸ“ ä¾é ¼è€…ç™ºè¨€ã‚’è¿½åŠ : {len(client_statements)}ä»¶")
                
                for statement in client_statements:
                    statement_id = statement.get('statement_id', 'ä¸æ˜')
                    date = statement.get('date')
                    statement_text = statement.get('statement', '')
                    context = statement.get('context', '')
                    
                    # èª¬æ˜ã‚’ä½œæˆ
                    description = f"ã€ä¾é ¼è€…ç™ºè¨€ã€‘\n{statement_text}"
                    if context:
                        description += f"\n\nã€çŠ¶æ³ã€‘\n{context}"
                    
                    # TimelineEventã‚’ä½œæˆ
                    event = TimelineEvent(
                        date=date,
                        evidence_id=statement_id,
                        evidence_number="ä¾é ¼è€…ç™ºè¨€",
                        description=description,
                        confidence="ç¢ºå®Ÿ" if date else "ä¸æ˜",
                        source_type="client_statement",
                        related_facts=[],
                        legal_significance={},
                        temporal_context=None
                    )
                    
                    timeline_events.append(event)
        
        # ã‚½ãƒ¼ãƒˆ
        timeline_events.sort(key=lambda e: e.sort_key)
        
        total_items = len(timeline_events)
        evidence_with_date = len(evidence_list) - no_date_count
        client_count = len([e for e in timeline_events if e.source_type == "client_statement"])
        
        print(f"\nâœ… æ™‚ç³»åˆ—æ§‹ç¯‰å®Œäº†")
        print(f"   - è¨¼æ‹ ï¼ˆæ—¥ä»˜ã‚ã‚Šï¼‰: {evidence_with_date}ä»¶")
        print(f"   - è¨¼æ‹ ï¼ˆæ—¥ä»˜ãªã—ï¼‰: {no_date_count}ä»¶")
        if client_count > 0:
            print(f"   - ä¾é ¼è€…ç™ºè¨€: {client_count}ä»¶")
        print(f"   - åˆè¨ˆ: {total_items}ä»¶")
        
        return timeline_events
    
    def generate_narrative(self, timeline_events: List[TimelineEvent]) -> str:
        """æ™‚ç³»åˆ—ã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰å®¢è¦³çš„ãªãƒŠãƒ©ãƒ†ã‚£ãƒ–ï¼ˆç‰©èªï¼‰ã‚’ç”Ÿæˆ
        
        Args:
            timeline_events: TimelineEventã®ãƒªã‚¹ãƒˆ
        
        Returns:
            å®¢è¦³çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not timeline_events:
            return "æ™‚ç³»åˆ—ã«åŸºã¥ãäº‹å®Ÿã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
        
        narrative_parts = []
        narrative_parts.append("=" * 80)
        narrative_parts.append("äº‹ä»¶ã®æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ï¼ˆå®¢è¦³çš„äº‹å®Ÿã®æµã‚Œï¼‰")
        narrative_parts.append("=" * 80)
        narrative_parts.append("")
        narrative_parts.append("â€» æ³•çš„åˆ¤æ–­ã‚’å«ã¾ãªã„ã€è¨¼æ‹ ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸå®¢è¦³çš„äº‹å®Ÿã®ã¿ã‚’æ™‚ç³»åˆ—ã§è¨˜è¼‰")
        narrative_parts.append("")
        
        # å¹´ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        events_by_year = defaultdict(list)
        events_no_date = []
        
        for event in timeline_events:
            if event.date:
                year = event.date.split('-')[0]
                events_by_year[year].append(event)
            else:
                events_no_date.append(event)
        
        # å¹´ã”ã¨ã«å‡ºåŠ›
        for year in sorted(events_by_year.keys()):
            narrative_parts.append(f"\nã€{year}å¹´ã€‘")
            narrative_parts.append("-" * 80)
            
            for event in events_by_year[year]:
                date_display = event.format_date_display()
                narrative_parts.append(f"\nâ—† {date_display} ({event.evidence_number})")
                
                # èª¬æ˜ã‚’æ•´å½¢ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆï¼‰
                description_lines = event.description.split('\n')
                for line in description_lines:
                    if line.strip():
                        narrative_parts.append(f"  {line.strip()}")
                
                narrative_parts.append("")
        
        # æ—¥ä»˜ä¸æ˜ã®è¨¼æ‹ 
        if events_no_date:
            narrative_parts.append("\nã€æ—¥ä»˜ä¸æ˜ã®è¨¼æ‹ ã€‘")
            narrative_parts.append("-" * 80)
            
            for event in events_no_date:
                narrative_parts.append(f"\nâ—† æ—¥ä»˜ä¸æ˜ ({event.evidence_number})")
                
                description_lines = event.description.split('\n')
                for line in description_lines:
                    if line.strip():
                        narrative_parts.append(f"  {line.strip()}")
                
                narrative_parts.append("")
        
        narrative_parts.append("\n" + "=" * 80)
        narrative_parts.append("æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼çµ‚äº†")
        narrative_parts.append("=" * 80)
        
        return "\n".join(narrative_parts)
    
    def generate_objective_narrative(self, timeline_events: List[TimelineEvent]) -> Dict:
        """AI ã‚’ä½¿ç”¨ã—ã¦å®¢è¦³çš„ãªæ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            timeline_events: TimelineEventã®ãƒªã‚¹ãƒˆ
        
        Returns:
            AI ãŒç”Ÿæˆã—ãŸå®¢è¦³çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼æƒ…å ±ã‚’å«ã‚€è¾æ›¸
            {
                "narrative": str,  # ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
                "fact_evidence_mapping": List[Dict],  # äº‹å®Ÿã¨è¨¼æ‹ ã®ç´ä»˜ã‘
                "key_facts": List[Dict]  # é‡è¦ãªäº‹å®Ÿã®ãƒªã‚¹ãƒˆ
            }
        """
        if not timeline_events:
            return {
                "narrative": "æ™‚ç³»åˆ—ã«åŸºã¥ãäº‹å®Ÿã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
                "fact_evidence_mapping": [],
                "key_facts": []
            }
        
        if not self.use_ai or not self.anthropic_client:
            print("âš ï¸ AI æ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™ã€‚åŸºæœ¬çš„ãªãƒŠãƒ©ãƒ†ã‚£ãƒ–ã‚’è¿”ã—ã¾ã™ã€‚")
            return {
                "narrative": self.generate_narrative(timeline_events),
                "fact_evidence_mapping": [],
                "key_facts": []
            }
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆã‚’ JSON å½¢å¼ã«å¤‰æ›
        timeline_data = [event.to_dict() for event in timeline_events]
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        prompt = self._create_enhanced_narrative_prompt(timeline_data)
        
        # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã¯å‰Šé™¤ï¼ˆä¸è¦ãªå†—é•·å‡ºåŠ›ã‚’é˜²ãï¼‰
        
        try:
            # Claude API ã‚’å‘¼ã³å‡ºã—
            response = self.anthropic_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=8000,
                temperature=0.3,
                system="ã‚ãªãŸã¯æ³•å¾‹æ–‡æ›¸ã®å°‚é–€å®¶ã§ã™ã€‚æä¾›ã•ã‚ŒãŸè¨¼æ‹ ã®æ™‚ç³»åˆ—æƒ…å ±ã‹ã‚‰ã€å®Œå…¨ã«å®¢è¦³çš„ã§ä¸­ç«‹çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚æ³•çš„åˆ¤æ–­ã‚„ä¸»è¦³çš„è©•ä¾¡ã¯ä¸€åˆ‡å«ã‚ãšã€è¨¼æ‹ ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹äº‹å®Ÿã®ã¿ã‚’æ™‚ç³»åˆ—é †ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€å„äº‹å®ŸãŒã©ã®è¨¼æ‹ ã«ã‚ˆã£ã¦è£ä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã‹ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚\n\nã€é‡è¦ã€‘å„è¨¼æ‹ ã®ã€æ—¥ä»˜è¡¨ç¤ºã€‘æ¬„ã«è¨˜è¼‰ã•ã‚ŒãŸæ—¥ä»˜ãŒã€ãã®è¨¼æ‹ ã®æ­£ç¢ºãªæ—¥ä»˜ã§ã™ã€‚ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆæ™‚ã¯ã€å¿…ãšã“ã®ã€æ—¥ä»˜è¡¨ç¤ºã€‘æ¬„ã®æ—¥ä»˜ã‚’ãã®ã¾ã¾ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
            result = self._parse_claude_response(response.content[0].text)
            
            print("âœ… Claude AI ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆå®Œäº†")
            return result
            
        except Exception as e:
            print(f"âš ï¸ AI ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print("åŸºæœ¬çš„ãªãƒŠãƒ©ãƒ†ã‚£ãƒ–ã‚’è¿”ã—ã¾ã™ã€‚")
            return {
                "narrative": self.generate_narrative(timeline_events),
                "fact_evidence_mapping": [],
                "key_facts": []
            }
    
    def _create_narrative_prompt(self, timeline_data: List[Dict]) -> str:
        """AI ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        
        Args:
            timeline_data: ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆã®è¾æ›¸ãƒªã‚¹ãƒˆ
        
        Returns:
            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        """
        prompt_parts = []
        prompt_parts.append("ä»¥ä¸‹ã®è¨¼æ‹ æƒ…å ±ã‹ã‚‰ã€æ™‚ç³»åˆ—ã«æ²¿ã£ãŸå®¢è¦³çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n")
        prompt_parts.append("ã€è¦ä»¶ã€‘")
        prompt_parts.append("1. å®Œå…¨ã«å®¢è¦³çš„ãƒ»ä¸­ç«‹çš„ãªè¨˜è¿°ï¼ˆæ³•çš„åˆ¤æ–­ã‚„è©•ä¾¡ã‚’å«ã‚ãªã„ï¼‰")
        prompt_parts.append("2. è¨¼æ‹ ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹äº‹å®Ÿã®ã¿ã‚’è¨˜è¿°")
        prompt_parts.append("3. æ™‚ç³»åˆ—é †ã«æ•´ç†ã•ã‚ŒãŸæµã‚Œ")
        prompt_parts.append("4. å„äº‹å®Ÿã®æ—¥ä»˜ã¨è¨¼æ‹ ç•ªå·ã‚’æ˜è¨˜")
        prompt_parts.append("5. äº‹å®Ÿé–“ã®å› æœé–¢ä¿‚ã¯å®¢è¦³çš„ã«è¨˜è¿°å¯èƒ½ãªç¯„å›²ã®ã¿")
        prompt_parts.append("6. èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã®æ–‡ç« å½¢å¼\n")
        prompt_parts.append("ã€è¨¼æ‹ æƒ…å ±ã€‘\n")
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        for event in timeline_data:
            date_display = event.get('date_display', 'æ—¥ä»˜ä¸æ˜')
            evidence_number = event.get('evidence_number', 'ä¸æ˜')
            description = event.get('description', '')
            
            prompt_parts.append(f"\nã€{date_display}ã€‘ ({evidence_number})")
            prompt_parts.append(description)
        
        prompt_parts.append("\n\nã€å‡ºåŠ›å½¢å¼ã€‘")
        prompt_parts.append("ä»¥ä¸‹ã®æ§‹é€ ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š")
        prompt_parts.append("")
        prompt_parts.append("# äº‹ä»¶ã®æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ï¼ˆå®¢è¦³çš„äº‹å®Ÿã®æµã‚Œï¼‰")
        prompt_parts.append("")
        prompt_parts.append("## æ¦‚è¦")
        prompt_parts.append("[äº‹ä»¶å…¨ä½“ã®æ¦‚è¦ã‚’2-3æ–‡ã§ç°¡æ½”ã«]")
        prompt_parts.append("")
        prompt_parts.append("## æ™‚ç³»åˆ—ã®æµã‚Œ")
        prompt_parts.append("")
        prompt_parts.append("### [æœŸé–“1]ï¼ˆä¾‹ï¼š2020å¹´ï¼‰")
        prompt_parts.append("[ã“ã®æœŸé–“ã®å‡ºæ¥äº‹ã‚’å®¢è¦³çš„ã«è¨˜è¿°]")
        prompt_parts.append("")
        prompt_parts.append("- YYYY-MM-DD: [äº‹å®Ÿã®è¨˜è¿°]ï¼ˆè¨¼æ‹ : ç”²XXï¼‰")
        prompt_parts.append("- YYYY-MM-DD: [äº‹å®Ÿã®è¨˜è¿°]ï¼ˆè¨¼æ‹ : ç”²YYï¼‰")
        prompt_parts.append("")
        prompt_parts.append("### [æœŸé–“2]")
        prompt_parts.append("...")
        prompt_parts.append("")
        prompt_parts.append("## ä¸»è¦ãªå‡ºæ¥äº‹ã®ã¾ã¨ã‚")
        prompt_parts.append("[æ™‚ç³»åˆ—ã§ç‰¹ã«é‡è¦ãªå‡ºæ¥äº‹ã‚’ç®‡æ¡æ›¸ãã§]")
        
        return "\n".join(prompt_parts)
    
    def _create_enhanced_narrative_prompt(self, timeline_data: List[Dict]) -> str:
        """AI ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆç”¨ã®æ‹¡å¼µãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆï¼ˆäº‹å®Ÿã¨è¨¼æ‹ ã®ç´ä»˜ã‘ä»˜ãï¼‰
        
        Args:
            timeline_data: ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚¤ãƒ™ãƒ³ãƒˆã®è¾æ›¸ãƒªã‚¹ãƒˆ
        
        Returns:
            ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        """
        prompt_parts = []
        prompt_parts.append("ä»¥ä¸‹ã®è¨¼æ‹ æƒ…å ±ã¨ä¾é ¼è€…ã‹ã‚‰ã®åŒ…æ‹¬çš„ãªæƒ…å ±ã‹ã‚‰ã€æ™‚ç³»åˆ—ã«æ²¿ã£ãŸå®¢è¦³çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n")
        
        # åŒ…æ‹¬çš„ãªä¾é ¼è€…ç™ºè¨€ã‚’è¿½åŠ 
        general_contexts = self._load_general_context()
        if general_contexts:
            prompt_parts.append("ã€ä¾é ¼è€…ã‹ã‚‰ã®åŒ…æ‹¬çš„ãªæƒ…å ±ã€‘")
            prompt_parts.append("â€» ã“ã‚Œã‚‰ã¯è¤‡æ•°ã®äº‹å®Ÿã«ã‚ãŸã‚‹å…¨ä½“çš„ãªæ–‡è„ˆæƒ…å ±ã§ã™ã€‚ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆã®éš›ã«è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚\n")
            
            for ctx in general_contexts:
                title = ctx.get('title', 'æƒ…å ±ãªã—')
                content = ctx.get('content', '')
                category = ctx.get('category', 'ä¸€èˆ¬')
                
                prompt_parts.append(f"\nã€{category}ã€‘{title}")
                prompt_parts.append(content)
            
            prompt_parts.append("\n" + "="*80 + "\n")
        
        prompt_parts.append("ã€è¦ä»¶ã€‘")
        prompt_parts.append("1. å®Œå…¨ã«å®¢è¦³çš„ãƒ»ä¸­ç«‹çš„ãªè¨˜è¿°ï¼ˆæ³•çš„åˆ¤æ–­ã‚„è©•ä¾¡ã‚’å«ã‚ãªã„ï¼‰")
        prompt_parts.append("2. è¨¼æ‹ ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹äº‹å®Ÿã®ã¿ã‚’è¨˜è¿°")
        prompt_parts.append("3. æ™‚ç³»åˆ—é †ã«æ•´ç†ã•ã‚ŒãŸæµã‚Œ")
        prompt_parts.append("4. å„äº‹å®Ÿã®æ—¥ä»˜ã¨è¨¼æ‹ ç•ªå·ã‚’æ˜è¨˜")
        prompt_parts.append("5. äº‹å®Ÿé–“ã®å› æœé–¢ä¿‚ã¯å®¢è¦³çš„ã«è¨˜è¿°å¯èƒ½ãªç¯„å›²ã®ã¿")
        prompt_parts.append("6. èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã®æ–‡ç« å½¢å¼")
        prompt_parts.append("7. **é‡è¦**: å„äº‹å®ŸãŒã©ã®è¨¼æ‹ ã«ã‚ˆã£ã¦è£ä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã‹ã‚’æ˜ç¢ºã«ç¤ºã™")
        prompt_parts.append("8. **é‡è¦**: ä¾é ¼è€…ã‹ã‚‰ã®åŒ…æ‹¬çš„æƒ…å ±ã‚’è€ƒæ…®ã—ã€å…¨ä½“ã®æ–‡è„ˆã‚’é©åˆ‡ã«åæ˜ ã™ã‚‹")
        prompt_parts.append("9. **ğŸš¨ æ¥µã‚ã¦é‡è¦**: å„è¨¼æ‹ ã®ã€æ—¥ä»˜è¡¨ç¤ºã€‘æ¬„ã«è¨˜è¼‰ã•ã‚ŒãŸæ—¥ä»˜ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨\n")
        prompt_parts.append("ã€è¨¼æ‹ æƒ…å ±ã€‘\n")
        
        # ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ï¼ˆè©³ç´°æƒ…å ±ã‚’å«ã‚€ï¼‰
        # é‡è¦: AIã«é€ã‚‹å‰ã«ã€æ··ä¹±ã‚’æ‹›ãæ™‚é–“æƒ…å ±ã‚’é™¤å¤–ã™ã‚‹
        for event in timeline_data:
            date_display = event.get('date_display', 'æ—¥ä»˜ä¸æ˜')
            evidence_number = event.get('evidence_number', 'ä¸æ˜')
            evidence_id = event.get('evidence_id', 'ä¸æ˜')
            description = event.get('description', '')
            source_type = event.get('source_type', 'evidence')
            related_facts = event.get('related_facts', [])
            legal_significance = event.get('legal_significance', {})
            temporal_context = event.get('temporal_context')
            
            # èª¬æ˜æ–‡ã‹ã‚‰æ™‚é–“çš„ãªæ··ä¹±ã‚’æ‹›ãè¡¨ç¾ã‚’é™¤å»
            cleaned_description = self._clean_temporal_references(description)
            
            prompt_parts.append(f"\nã€æ—¥ä»˜è¡¨ç¤ºã€‘: {date_display}")
            prompt_parts.append(f"ã€è¨¼æ‹ ç•ªå·ã€‘: {evidence_number}")
            prompt_parts.append(f"ã€è¨¼æ‹ IDã€‘: {evidence_id}")
            prompt_parts.append(f"ã€æƒ…å ±æºã€‘: {source_type}")
            prompt_parts.append(f"ã€å†…å®¹ã€‘: {cleaned_description}")
            
            # related_factsã‚’è¿½åŠ ï¼ˆæ™‚é–“çš„è¡¨ç¾ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
            if related_facts:
                prompt_parts.append("\nã€é–¢é€£ã™ã‚‹äº‹å®Ÿã€‘")
                for fact in related_facts:
                    fact_type = fact.get('type', 'ä¸æ˜')
                    fact_content = fact.get('content', '')
                    cleaned_fact = self._clean_temporal_references(fact_content)
                    prompt_parts.append(f"  - [{fact_type}] {cleaned_fact}")
            
            # temporal_contextã‚’è¿½åŠ ï¼ˆæ™‚é–“çš„è¡¨ç¾ã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
            if temporal_context:
                cleaned_context = self._clean_temporal_references(temporal_context)
                prompt_parts.append(f"\nã€æ™‚ç³»åˆ—çš„ãªæ–‡è„ˆã€‘")
                prompt_parts.append(f"  {cleaned_context}")
            
            # legal_significanceã‚’è¿½åŠ ï¼ˆé‡è¦ãªå ´åˆã®ã¿ï¼‰
            if legal_significance and legal_significance.get('key_legal_points'):
                prompt_parts.append(f"\nã€æ³•çš„ãªãƒã‚¤ãƒ³ãƒˆã€‘")
                for point in legal_significance.get('key_legal_points', []):
                    prompt_parts.append(f"  - {point}")
        
        prompt_parts.append("\n\nã€å‡ºåŠ›å½¢å¼ã€‘")
        prompt_parts.append("ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š")
        prompt_parts.append("")
        prompt_parts.append("```json")
        prompt_parts.append("{")
        prompt_parts.append('  "narrative": "# äº‹ä»¶ã®æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ï¼ˆå®¢è¦³çš„äº‹å®Ÿã®æµã‚Œï¼‰\\n\\n## æ¦‚è¦\\n[äº‹ä»¶å…¨ä½“ã®æ¦‚è¦ã‚’2-3æ–‡ã§ç°¡æ½”ã«]\\n\\n## æ™‚ç³»åˆ—ã®æµã‚Œ\\n\\n### [æœŸé–“1]ï¼ˆä¾‹ï¼š2020å¹´ï¼‰\\n[ã“ã®æœŸé–“ã®å‡ºæ¥äº‹ã‚’å®¢è¦³çš„ã«è¨˜è¿°]\\n\\n- YYYY-MM-DD: [äº‹å®Ÿã®è¨˜è¿°]ï¼ˆè¨¼æ‹ : ç”²XXï¼‰\\n- YYYY-MM-DD: [äº‹å®Ÿã®è¨˜è¿°]ï¼ˆè¨¼æ‹ : ç”²YYï¼‰\\n\\n### [æœŸé–“2]\\n...\\n\\n## ä¸»è¦ãªå‡ºæ¥äº‹ã®ã¾ã¨ã‚\\n[æ™‚ç³»åˆ—ã§ç‰¹ã«é‡è¦ãªå‡ºæ¥äº‹ã‚’ç®‡æ¡æ›¸ãã§]",')
        prompt_parts.append('  "fact_evidence_mapping": [')
        prompt_parts.append('    {')
        prompt_parts.append('      "fact_id": "fact_001",')
        prompt_parts.append('      "fact_description": "å…·ä½“çš„ãªäº‹å®Ÿã®è¨˜è¿°",')
        prompt_parts.append('      "date": "YYYY-MM-DD",')
        prompt_parts.append('      "supporting_evidence": ["ko001", "ko002"],')
        prompt_parts.append('      "evidence_numbers": ["ç”²001", "ç”²002"],')
        prompt_parts.append('      "confidence": "high"')
        prompt_parts.append('    },')
        prompt_parts.append('    ...')
        prompt_parts.append('  ],')
        prompt_parts.append('  "key_facts": [')
        prompt_parts.append('    {')
        prompt_parts.append('      "fact_id": "fact_001",')
        prompt_parts.append('      "importance": "high",')
        prompt_parts.append('      "summary": "é‡è¦ãªäº‹å®Ÿã®è¦ç´„"')
        prompt_parts.append('    },')
        prompt_parts.append('    ...')
        prompt_parts.append('  ]')
        prompt_parts.append("}")
        prompt_parts.append("```")
        prompt_parts.append("")
        prompt_parts.append("**é‡è¦äº‹é …**:")
        prompt_parts.append("- fact_evidence_mappingã«ã¯ã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å†…ã®å„é‡è¦ãªäº‹å®Ÿã‚’è¨˜è¼‰")
        prompt_parts.append("- å„äº‹å®Ÿã«ã¯ã€ãã‚Œã‚’è£ä»˜ã‘ã‚‹è¨¼æ‹ ã®IDã¨è¨¼æ‹ ç•ªå·ã‚’æ˜è¨˜")
        prompt_parts.append("- confidenceã¯ã€ãã®äº‹å®Ÿã®ç¢ºå®Ÿæ€§ï¼ˆhigh/medium/lowï¼‰")
        prompt_parts.append("- key_factsã«ã¯ã€ç‰¹ã«é‡è¦ãªäº‹å®Ÿã®ã¿ã‚’æŠ½å‡º")
        prompt_parts.append("- **æ—¥ä»˜ã¯ã€è¨¼æ‹ æƒ…å ±ã€‘ã®ã€æ—¥ä»˜è¡¨ç¤ºã€‘æ¬„ã«è¨˜è¼‰ã•ã‚ŒãŸã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ç”¨ã™ã‚‹ã“ã¨**")
        
        return "\n".join(prompt_parts)
    
    def _parse_claude_response(self, response_text: str) -> Dict:
        """Claude ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        
        Args:
            response_text: Claude ã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸè¾æ›¸ãƒ‡ãƒ¼ã‚¿
        """
        try:
            # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            import re
            json_match = re.search(r'```json\s*({.*?})\s*```', response_text, re.DOTALL)
            
            if json_match:
                json_str = json_match.group(1)
                result = json.loads(json_str)
                return result
            else:
                # JSONãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‚’narrativeã¨ã—ã¦æ‰±ã†
                return {
                    "narrative": response_text,
                    "fact_evidence_mapping": [],
                    "key_facts": []
                }
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "narrative": response_text,
                "fact_evidence_mapping": [],
                "key_facts": []
            }
        except Exception as e:
            print(f"âš ï¸ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "narrative": response_text,
                "fact_evidence_mapping": [],
                "key_facts": []
            }
    
    def refine_narrative_with_instruction(self, current_result: Dict, 
                                         user_instruction: str) -> Dict:
        """è‡ªç„¶è¨€èªã«ã‚ˆã‚‹æŒ‡ç¤ºã§ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ”¹å–„
        
        Args:
            current_result: ç¾åœ¨ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ç”Ÿæˆçµæœ
            user_instruction: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è‡ªç„¶è¨€èªã«ã‚ˆã‚‹æŒ‡ç¤º
        
        Returns:
            æ”¹å–„ã•ã‚ŒãŸã‚¹ãƒˆãƒ¼ãƒªãƒ¼æƒ…å ±
        """
        if not self.use_ai or not self.anthropic_client:
            print("âš ï¸ AI æ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™ã€‚æ”¹å–„ã§ãã¾ã›ã‚“ã€‚")
            return current_result
        
        print("\nğŸ”„ ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºã«åŸºã¥ã„ã¦ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ”¹å–„ä¸­...")
        
        # æ”¹å–„ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        prompt = f"""ä»¥ä¸‹ã®æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¾“ã£ã¦æ”¹å–„ã—ã¦ãã ã•ã„ã€‚

ã€ç¾åœ¨ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã€‘
{current_result.get('narrative', '')}

ã€äº‹å®Ÿã¨è¨¼æ‹ ã®ç´ä»˜ã‘ã€‘
{json.dumps(current_result.get('fact_evidence_mapping', []), ensure_ascii=False, indent=2)}

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æ”¹å–„æŒ‡ç¤ºã€‘
{user_instruction}

ã€è¦ä»¶ã€‘
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¾“ã£ã¦å†…å®¹ã‚’ä¿®æ­£ãƒ»æ”¹å–„
2. å®Œå…¨ã«å®¢è¦³çš„ãƒ»ä¸­ç«‹çš„ãªè¨˜è¿°ã‚’ç¶­æŒ
3. æ³•çš„åˆ¤æ–­ã‚„ä¸»è¦³çš„è©•ä¾¡ã¯å«ã‚ãªã„
4. äº‹å®Ÿã¨è¨¼æ‹ ã®ç´ä»˜ã‘ã‚’æ˜ç¢ºã«ä¿æŒ
5. æ”¹å–„ç®‡æ‰€ã‚’æ˜ç¤º
6. **ğŸš¨ æ¥µã‚ã¦é‡è¦**: æ—¢å­˜ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ—¥ä»˜ã‚’å¤‰æ›´ã—ã¦ã¯ãªã‚‰ãªã„
   - å…ƒã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æ—¥ä»˜ã¯æ­£ç¢ºã«æ¤œè¨¼æ¸ˆã¿ãªã®ã§ã€å¿…ãšãã®ã¾ã¾ç¶­æŒã™ã‚‹ã“ã¨
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã§æ—¥ä»˜ã®å¤‰æ›´ã‚’æ˜ç¤ºçš„ã«è¦æ±‚ã•ã‚Œãªã„é™ã‚Šã€æ—¥ä»˜ã¯å¤‰æ›´ã—ãªã„

ã€å‡ºåŠ›å½¢å¼ã€‘
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

```json
{{
  "narrative": "æ”¹å–„å¾Œã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å…¨æ–‡",
  "fact_evidence_mapping": [...],
  "key_facts": [...],
  "changes_made": "å®Ÿæ–½ã—ãŸå¤‰æ›´å†…å®¹ã®èª¬æ˜"
}}
```
"""
        
        try:
            response = self.anthropic_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=8000,
                temperature=0.3,
                system="ã‚ãªãŸã¯æ³•å¾‹æ–‡æ›¸ã®å°‚é–€å®¶ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¾“ã£ã¦æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ”¹å–„ã—ã¾ã™ãŒã€å®¢è¦³æ€§ã¨ä¸­ç«‹æ€§ã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚é‡è¦: æ—¢å­˜ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ—¥ä»˜ã¯æ­£ç¢ºã«æ¤œè¨¼æ¸ˆã¿ãªã®ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ˜ç¤ºçš„ã«æ—¥ä»˜ã®å¤‰æ›´ã‚’æŒ‡ç¤ºã—ãªã„é™ã‚Šã€çµ¶å¯¾ã«å¤‰æ›´ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚",
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            result = self._parse_claude_response(response.content[0].text)
            
            print("âœ… ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æ”¹å–„ãŒå®Œäº†ã—ã¾ã—ãŸ")
            if "changes_made" in result:
                print(f"\nã€å®Ÿæ–½ã—ãŸå¤‰æ›´ã€‘\n{result['changes_made']}")
            
            return result
            
        except Exception as e:
            print(f"âš ï¸ ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ”¹å–„ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return current_result
    
    def analyze_evidence_relationships(self, timeline_events: List[TimelineEvent]) -> Dict:
        """è¨¼æ‹ é–“ã®é–¢é€£æ€§ã‚’åˆ†æ
        
        Args:
            timeline_events: TimelineEventã®ãƒªã‚¹ãƒˆ
        
        Returns:
            é–¢é€£æ€§åˆ†æçµæœã®è¾æ›¸
        """
        print("\nğŸ“Š è¨¼æ‹ é–“ã®é–¢é€£æ€§ã‚’åˆ†æä¸­...")
        
        relationships = {
            "temporal_clusters": [],  # æ™‚é–“çš„ã«è¿‘æ¥ã—ãŸè¨¼æ‹ ç¾¤
            "theme_clusters": [],     # ãƒ†ãƒ¼ãƒåˆ¥ã®è¨¼æ‹ ç¾¤
            "chronological_gaps": []  # æ™‚ç³»åˆ—ä¸Šã®ã‚®ãƒ£ãƒƒãƒ—
        }
        
        # æ™‚é–“çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆ1ãƒ¶æœˆä»¥å†…ã®è¨¼æ‹ ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
        current_cluster = []
        last_date = None
        
        for event in timeline_events:
            if not event.date:
                continue
            
            if last_date is None:
                current_cluster = [event]
                last_date = event.date
            else:
                # æ—¥ä»˜ã®å·®åˆ†ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                date_diff = self._calculate_date_diff(last_date, event.date)
                
                if date_diff <= 31:  # 1ãƒ¶æœˆä»¥å†…
                    current_cluster.append(event)
                else:
                    if len(current_cluster) > 1:
                        relationships["temporal_clusters"].append({
                            "period": f"{current_cluster[0].date} - {current_cluster[-1].date}",
                            "evidence_count": len(current_cluster),
                            "evidence_numbers": [e.evidence_number for e in current_cluster]
                        })
                    
                    # ã‚®ãƒ£ãƒƒãƒ—ã‚’è¨˜éŒ²ï¼ˆ3ãƒ¶æœˆä»¥ä¸Šï¼‰
                    if date_diff >= 90:
                        relationships["chronological_gaps"].append({
                            "from_date": last_date,
                            "to_date": event.date,
                            "gap_days": date_diff,
                            "description": f"{last_date} ã‹ã‚‰ {event.date} ã¾ã§ç´„ {date_diff} æ—¥é–“ã®ã‚®ãƒ£ãƒƒãƒ—"
                        })
                    
                    current_cluster = [event]
                
                last_date = event.date
        
        # æœ€å¾Œã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¿½åŠ 
        if len(current_cluster) > 1:
            relationships["temporal_clusters"].append({
                "period": f"{current_cluster[0].date} - {current_cluster[-1].date}",
                "evidence_count": len(current_cluster),
                "evidence_numbers": [e.evidence_number for e in current_cluster]
            })
        
        print(f"  - æ™‚é–“çš„ã‚¯ãƒ©ã‚¹ã‚¿: {len(relationships['temporal_clusters'])}ä»¶")
        print(f"  - æ™‚ç³»åˆ—ã‚®ãƒ£ãƒƒãƒ—: {len(relationships['chronological_gaps'])}ä»¶")
        
        return relationships
    
    def _calculate_date_diff(self, date1: str, date2: str) -> int:
        """2ã¤ã®æ—¥ä»˜é–“ã®æ—¥æ•°ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            date1: æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰
            date2: æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰
        
        Returns:
            æ—¥æ•°ã®å·®åˆ†
        """
        try:
            d1 = datetime.strptime(date1, "%Y-%m-%d")
            d2 = datetime.strptime(date2, "%Y-%m-%d")
            return abs((d2 - d1).days)
        except:
            # ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯å¤§ããªå€¤ã‚’è¿”ã™
            return 9999
    
    def export_timeline(self, timeline_events: List[TimelineEvent], 
                       output_format: str = "json",
                       include_ai_narrative: bool = True) -> Optional[str]:
        """ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆGoogle Driveç›´æ¥ä¿å­˜ï¼‰
        
        Args:
            timeline_events: TimelineEventã®ãƒªã‚¹ãƒˆ
            output_format: å‡ºåŠ›å½¢å¼ï¼ˆ"json", "markdown", "text", "html"ï¼‰
            include_ai_narrative: AI ç”Ÿæˆã®ãƒŠãƒ©ãƒ†ã‚£ãƒ–ã‚’å«ã‚ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        
        Returns:
            Google Driveã®URL
        """
        if not timeline_events:
            print("âš ï¸ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # AI ãƒŠãƒ©ãƒ†ã‚£ãƒ–ã‚’ç”Ÿæˆï¼ˆå¿…è¦ãªå ´åˆï¼‰
        ai_narrative = None
        if include_ai_narrative and self.use_ai:
            ai_narrative = self.generate_objective_narrative(timeline_events)
        
        # è¨¼æ‹ é–“ã®é–¢é€£æ€§ã‚’åˆ†æ
        relationships = self.analyze_evidence_relationships(timeline_events)
        
        if output_format == "json":
            return self._export_json(timeline_events, timestamp, ai_narrative, relationships)
        elif output_format == "markdown":
            return self._export_markdown(timeline_events, timestamp, ai_narrative, relationships)
        elif output_format == "text":
            return self._export_text(timeline_events, timestamp, ai_narrative)
        elif output_format == "html":
            return self._export_html(timeline_events, timestamp, ai_narrative, relationships)
        else:
            print(f"âŒ æœªå¯¾å¿œã®å‡ºåŠ›å½¢å¼: {output_format}")
            return None
    
    def _export_json(self, timeline_events: List[TimelineEvent], 
                    timestamp: str, ai_narrative: Optional[Dict], 
                    relationships: Dict) -> Optional[str]:
        """JSONå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆGoogle Driveç›´æ¥ä¿å­˜ï¼‰"""
        import tempfile
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_file = tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', suffix='.json', delete=False)
        output_path = temp_file.name
        
        case_id = self.current_case.get('case_id', 'unknown')
        
        timeline_data = {
            "metadata": {
                "case_id": case_id,
                "case_name": self.current_case.get('case_name', 'ä¸æ˜'),
                "generated_at": datetime.now().isoformat(),
                "total_events": len(timeline_events),
                "ai_generated": ai_narrative is not None
            },
            "timeline": [event.to_dict() for event in timeline_events],
            "relationships": relationships
        }
        
        if ai_narrative:
            # ai_narrativeã¯è¾æ›¸å½¢å¼ï¼ˆnarrative, fact_evidence_mapping, key_factsï¼‰
            timeline_data["ai_narrative"] = ai_narrative.get("narrative", "")
            timeline_data["fact_evidence_mapping"] = ai_narrative.get("fact_evidence_mapping", [])
            timeline_data["key_facts"] = ai_narrative.get("key_facts", [])
        
        with temp_file as f:
            json.dump(timeline_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… JSONå½¢å¼ã§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        
        # Google Driveã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        file_name = f"timeline_{timestamp}.json"
        print(f"\nğŸ“¤ Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        gdrive_url = self._upload_to_gdrive(output_path, file_name)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        try:
            os.remove(output_path)
        except:
            pass
        
        return gdrive_url
    
    def _export_markdown(self, timeline_events: List[TimelineEvent],
                        timestamp: str, ai_narrative: Optional[Dict],
                        relationships: Dict) -> Optional[str]:
        """Markdownå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆGoogle Driveç›´æ¥ä¿å­˜ï¼‰"""
        import tempfile
        
        temp_file = tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', suffix='.md', delete=False)
        output_path = temp_file.name
        
        case_id = self.current_case.get('case_id', 'unknown')
        
        md_lines = []
        md_lines.append(f"# äº‹ä»¶ã®æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼")
        md_lines.append(f"")
        md_lines.append(f"**äº‹ä»¶ID**: {case_id}")
        md_lines.append(f"**äº‹ä»¶å**: {self.current_case.get('case_name', 'ä¸æ˜')}")
        md_lines.append(f"**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        md_lines.append(f"**è¨¼æ‹ ä»¶æ•°**: {len(timeline_events)}ä»¶")
        md_lines.append(f"")
        md_lines.append(f"---")
        md_lines.append(f"")
        
        # AI ãƒŠãƒ©ãƒ†ã‚£ãƒ–ã‚’è¿½åŠ 
        if ai_narrative:
            md_lines.append(f"## AI ç”Ÿæˆã®å®¢è¦³çš„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼")
            md_lines.append(f"")
            md_lines.append(ai_narrative.get("narrative", ""))
            md_lines.append(f"")
            
            # äº‹å®Ÿã¨è¨¼æ‹ ã®ç´ä»˜ã‘ã‚’è¿½åŠ 
            fact_mapping = ai_narrative.get("fact_evidence_mapping", [])
            if fact_mapping:
                md_lines.append(f"## äº‹å®Ÿã¨è¨¼æ‹ ã®ç´ä»˜ã‘")
                md_lines.append(f"")
                for fact in fact_mapping:
                    md_lines.append(f"### {fact.get('fact_description', 'ä¸æ˜ãªäº‹å®Ÿ')}")
                    md_lines.append(f"")
                    md_lines.append(f"- **æ—¥ä»˜**: {fact.get('date', 'ä¸æ˜')}")
                    md_lines.append(f"- **è£ä»˜ã‘è¨¼æ‹ **: {', '.join(fact.get('evidence_numbers', []))}")
                    md_lines.append(f"- **ç¢ºå®Ÿæ€§**: {fact.get('confidence', 'unknown')}")
                    md_lines.append(f"")
            
            md_lines.append(f"---")
            md_lines.append(f"")
        
        # è¨¼æ‹ é–“ã®é–¢é€£æ€§æƒ…å ±
        if relationships["temporal_clusters"]:
            md_lines.append(f"## æ™‚é–“çš„ãªè¨¼æ‹ ã‚°ãƒ«ãƒ¼ãƒ—")
            md_lines.append(f"")
            for cluster in relationships["temporal_clusters"]:
                md_lines.append(f"- **{cluster['period']}**: {cluster['evidence_count']}ä»¶ã®è¨¼æ‹ ")
                md_lines.append(f"  - {', '.join(cluster['evidence_numbers'])}")
            md_lines.append(f"")
        
        if relationships["chronological_gaps"]:
            md_lines.append(f"## æ™‚ç³»åˆ—ä¸Šã®ã‚®ãƒ£ãƒƒãƒ—")
            md_lines.append(f"")
            for gap in relationships["chronological_gaps"]:
                md_lines.append(f"- {gap['description']}")
            md_lines.append(f"")
        
        md_lines.append(f"## è©³ç´°ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿")
        md_lines.append(f"")
        
        # å¹´ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        events_by_year = defaultdict(list)
        events_no_date = []
        
        for event in timeline_events:
            if event.date:
                year = event.date.split('-')[0]
                events_by_year[year].append(event)
            else:
                events_no_date.append(event)
        
        for year in sorted(events_by_year.keys()):
            md_lines.append(f"### {year}å¹´")
            md_lines.append(f"")
            
            for event in events_by_year[year]:
                date_display = event.format_date_display()
                md_lines.append(f"#### {date_display} ({event.evidence_number})")
                md_lines.append(f"")
                md_lines.append(event.description)
                md_lines.append(f"")
        
        if events_no_date:
            md_lines.append(f"### æ—¥ä»˜ä¸æ˜ã®è¨¼æ‹ ")
            md_lines.append(f"")
            
            for event in events_no_date:
                md_lines.append(f"#### ({event.evidence_number})")
                md_lines.append(f"")
                md_lines.append(event.description)
                md_lines.append(f"")
        
        with temp_file as f:
            f.write('\n'.join(md_lines))
        
        print(f"\nâœ… Markdownå½¢å¼ã§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        
        # Google Driveã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        file_name = f"timeline_{timestamp}.md"
        print(f"\nğŸ“¤ Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        gdrive_url = self._upload_to_gdrive(output_path, file_name)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        try:
            os.remove(output_path)
        except:
            pass
        
        return gdrive_url
    
    def _export_text(self, timeline_events: List[TimelineEvent],
                    timestamp: str, ai_narrative: Optional[Dict]) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆGoogle Driveç›´æ¥ä¿å­˜ï¼‰"""
        import tempfile
        
        temp_file = tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', suffix='.txt', delete=False)
        output_path = temp_file.name
        
        text_parts = []
        
        # AI ãƒŠãƒ©ãƒ†ã‚£ãƒ–ãŒã‚ã‚Œã°è¿½åŠ 
        if ai_narrative:
            text_parts.append("=" * 80)
            text_parts.append("AI ç”Ÿæˆã®å®¢è¦³çš„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼")
            text_parts.append("=" * 80)
            text_parts.append("")
            text_parts.append(ai_narrative.get("narrative", ""))
            text_parts.append("")
            text_parts.append("\n" + "=" * 80)
            text_parts.append("è©³ç´°ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿")
            text_parts.append("=" * 80)
            text_parts.append("")
        
        # åŸºæœ¬çš„ãªãƒŠãƒ©ãƒ†ã‚£ãƒ–ã‚’è¿½åŠ 
        narrative = self.generate_narrative(timeline_events)
        text_parts.append(narrative)
        
        with temp_file as f:
            f.write('\n'.join(text_parts))
        
        print(f"\nâœ… ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        
        # Google Driveã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        file_name = f"timeline_{timestamp}.txt"
        print(f"\nğŸ“¤ Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        gdrive_url = self._upload_to_gdrive(output_path, file_name)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        try:
            os.remove(output_path)
        except:
            pass
        
        return gdrive_url
    
    def _export_html(self, timeline_events: List[TimelineEvent],
                    timestamp: str, ai_narrative: Optional[str],
                    relationships: Dict) -> Optional[str]:
        """HTMLå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆGoogle Driveç›´æ¥ä¿å­˜ï¼‰"""
        import tempfile
        
        temp_file = tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', suffix='.html', delete=False)
        output_path = temp_file.name
        
        case_id = self.current_case.get('case_id', 'unknown')
        case_name = self.current_case.get('case_name', 'ä¸æ˜')
        
        html_parts = []
        html_parts.append("<!DOCTYPE html>")
        html_parts.append("<html lang='ja'>")
        html_parts.append("<head>")
        html_parts.append("<meta charset='UTF-8'>")
        html_parts.append("<meta name='viewport' content='width=device-width, initial-scale=1.0'>")
        html_parts.append(f"<title>æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ - {case_name}</title>")
        html_parts.append("<style>")
        html_parts.append("""
            body {
                font-family: 'Hiragino Kaku Gothic Pro', 'Meiryo', sans-serif;
                line-height: 1.8;
                max-width: 1200px;
                margin: 0 auto;
                padding: 20px;
                background-color: #f5f5f5;
            }
            .header {
                background-color: #2c3e50;
                color: white;
                padding: 30px;
                border-radius: 10px;
                margin-bottom: 30px;
            }
            .header h1 {
                margin: 0 0 10px 0;
            }
            .metadata {
                font-size: 0.9em;
                opacity: 0.9;
            }
            .section {
                background-color: white;
                padding: 30px;
                margin-bottom: 20px;
                border-radius: 10px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }
            .section h2 {
                color: #2c3e50;
                border-bottom: 2px solid #3498db;
                padding-bottom: 10px;
                margin-top: 0;
            }
            .ai-narrative {
                background-color: #ecf0f1;
                padding: 20px;
                border-left: 4px solid #3498db;
                margin: 20px 0;
                white-space: pre-wrap;
            }
            .timeline-event {
                margin-bottom: 30px;
                padding: 20px;
                background-color: #f9f9f9;
                border-left: 4px solid #27ae60;
                border-radius: 5px;
            }
            .timeline-event h3 {
                margin-top: 0;
                color: #27ae60;
            }
            .evidence-number {
                display: inline-block;
                background-color: #3498db;
                color: white;
                padding: 3px 10px;
                border-radius: 3px;
                font-size: 0.9em;
                margin-left: 10px;
            }
            .description {
                margin-top: 15px;
                color: #34495e;
            }
            .cluster-info {
                background-color: #fff3cd;
                padding: 15px;
                border-radius: 5px;
                margin: 10px 0;
            }
            .gap-info {
                background-color: #f8d7da;
                padding: 15px;
                border-radius: 5px;
                margin: 10px 0;
            }
            .year-section {
                margin-top: 40px;
            }
            .year-header {
                background-color: #34495e;
                color: white;
                padding: 15px 20px;
                border-radius: 5px;
                margin-bottom: 20px;
            }
        """)
        html_parts.append("</style>")
        html_parts.append("</head>")
        html_parts.append("<body>")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        html_parts.append("<div class='header'>")
        html_parts.append(f"<h1>äº‹ä»¶ã®æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼</h1>")
        html_parts.append("<div class='metadata'>")
        html_parts.append(f"<strong>äº‹ä»¶ID:</strong> {case_id}<br>")
        html_parts.append(f"<strong>äº‹ä»¶å:</strong> {case_name}<br>")
        html_parts.append(f"<strong>ç”Ÿæˆæ—¥æ™‚:</strong> {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}<br>")
        html_parts.append(f"<strong>è¨¼æ‹ ä»¶æ•°:</strong> {len(timeline_events)}ä»¶")
        html_parts.append("</div>")
        html_parts.append("</div>")
        
        # AI ãƒŠãƒ©ãƒ†ã‚£ãƒ–
        if ai_narrative:
            html_parts.append("<div class='section'>")
            html_parts.append("<h2>ğŸ¤– AI ç”Ÿæˆã®å®¢è¦³çš„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼</h2>")
            html_parts.append(f"<div class='ai-narrative'>{ai_narrative}</div>")
            html_parts.append("</div>")
        
        # è¨¼æ‹ é–“ã®é–¢é€£æ€§
        if relationships["temporal_clusters"] or relationships["chronological_gaps"]:
            html_parts.append("<div class='section'>")
            html_parts.append("<h2>ğŸ“Š è¨¼æ‹ é–“ã®é–¢é€£æ€§åˆ†æ</h2>")
            
            if relationships["temporal_clusters"]:
                html_parts.append("<h3>æ™‚é–“çš„ãªè¨¼æ‹ ã‚°ãƒ«ãƒ¼ãƒ—</h3>")
                for cluster in relationships["temporal_clusters"]:
                    html_parts.append("<div class='cluster-info'>")
                    html_parts.append(f"<strong>{cluster['period']}</strong>: {cluster['evidence_count']}ä»¶ã®è¨¼æ‹ <br>")
                    html_parts.append(f"è¨¼æ‹ ç•ªå·: {', '.join(cluster['evidence_numbers'])}")
                    html_parts.append("</div>")
            
            if relationships["chronological_gaps"]:
                html_parts.append("<h3>æ™‚ç³»åˆ—ä¸Šã®ã‚®ãƒ£ãƒƒãƒ—</h3>")
                for gap in relationships["chronological_gaps"]:
                    html_parts.append("<div class='gap-info'>")
                    html_parts.append(gap['description'])
                    html_parts.append("</div>")
            
            html_parts.append("</div>")
        
        # è©³ç´°ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
        html_parts.append("<div class='section'>")
        html_parts.append("<h2>ğŸ“… è©³ç´°ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿</h2>")
        
        # å¹´ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        events_by_year = defaultdict(list)
        events_no_date = []
        
        for event in timeline_events:
            if event.date:
                year = event.date.split('-')[0]
                events_by_year[year].append(event)
            else:
                events_no_date.append(event)
        
        for year in sorted(events_by_year.keys()):
            html_parts.append("<div class='year-section'>")
            html_parts.append(f"<div class='year-header'><h3>{year}å¹´</h3></div>")
            
            for event in events_by_year[year]:
                date_display = event.format_date_display()
                html_parts.append("<div class='timeline-event'>")
                html_parts.append(f"<h3>{date_display}<span class='evidence-number'>{event.evidence_number}</span></h3>")
                html_parts.append(f"<div class='description'>{event.description.replace(chr(10), '<br>')}</div>")
                html_parts.append("</div>")
            
            html_parts.append("</div>")
        
        if events_no_date:
            html_parts.append("<div class='year-section'>")
            html_parts.append("<div class='year-header'><h3>æ—¥ä»˜ä¸æ˜ã®è¨¼æ‹ </h3></div>")
            
            for event in events_no_date:
                html_parts.append("<div class='timeline-event'>")
                html_parts.append(f"<h3>æ—¥ä»˜ä¸æ˜<span class='evidence-number'>{event.evidence_number}</span></h3>")
                html_parts.append(f"<div class='description'>{event.description.replace(chr(10), '<br>')}</div>")
                html_parts.append("</div>")
            
            html_parts.append("</div>")
        
        html_parts.append("</div>")
        
        html_parts.append("</body>")
        html_parts.append("</html>")
        
        with temp_file as f:
            f.write('\n'.join(html_parts))
        
        print(f"\nâœ… HTMLå½¢å¼ã§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        
        # Google Driveã«ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        file_name = f"timeline_{timestamp}.html"
        print(f"\nğŸ“¤ Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
        gdrive_url = self._upload_to_gdrive(output_path, file_name)
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        try:
            os.remove(output_path)
        except:
            pass
        
        return gdrive_url
    
    def _upload_to_gdrive(self, local_file_path: str, file_name: str) -> Optional[str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Google Driveã®äº‹ä»¶ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        
        Args:
            local_file_path: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            file_name: Google Driveä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«å
        
        Returns:
            Google Driveã®ãƒ•ã‚¡ã‚¤ãƒ«IDï¼ˆæˆåŠŸæ™‚ï¼‰ã€Noneï¼ˆå¤±æ•—æ™‚ï¼‰
        """
        try:
            # Google Drive ã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—
            service = self.case_manager.get_google_drive_service()
            if not service:
                print("âš ï¸ Google Drive ã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
                return None
            
            # äº‹ä»¶ãƒ•ã‚©ãƒ«ãƒ€IDã‚’å–å¾—
            case_folder_id = self.current_case.get('case_folder_id')
            if not case_folder_id:
                print("âš ï¸ äº‹ä»¶ãƒ•ã‚©ãƒ«ãƒ€IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Google Driveã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return None
            
            # ãƒ•ã‚©ãƒ«ãƒ€IDã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if not case_folder_id or len(case_folder_id) < 20:
                print(f"âš ï¸ äº‹ä»¶ãƒ•ã‚©ãƒ«ãƒ€IDãŒç„¡åŠ¹ã§ã™: {case_folder_id}")
                print("   Google Driveã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return None
            
            # timelineã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¢ã™ or ä½œæˆ
            timeline_folder_id = self._find_or_create_timeline_folder(service, case_folder_id)
            if not timeline_folder_id:
                print("âš ï¸ timelineãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚Google Driveã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return None
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®MIMEã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            mime_type = self._get_mime_type(file_name)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            file_metadata = {
                'name': file_name,
                'parents': [timeline_folder_id]
            }
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–å¯¾å¿œï¼‰
            media = MediaFileUpload(local_file_path, mimetype=mime_type, resumable=True)
            uploaded_file = service.files().create(
                body=file_metadata,
                media_body=media,
                supportsAllDrives=True,
                fields='id, name, webViewLink'
            ).execute()
            
            file_id = uploaded_file.get('id')
            web_link = uploaded_file.get('webViewLink')
            
            print(f"âœ… Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            print(f"   ğŸ“ ãƒªãƒ³ã‚¯: {web_link}")
            
            return web_link
            
        except Exception as e:
            print(f"âš ï¸ Google Driveã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None
    
    def _find_or_create_timeline_folder(self, service, parent_folder_id: str) -> Optional[str]:
        """timelineã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¢ã™ã€ãªã‘ã‚Œã°ä½œæˆ
        
        Args:
            service: Google Drive ã‚µãƒ¼ãƒ“ã‚¹
            parent_folder_id: è¦ªãƒ•ã‚©ãƒ«ãƒ€ï¼ˆäº‹ä»¶ãƒ•ã‚©ãƒ«ãƒ€ï¼‰ã®ID
        
        Returns:
            timelineãƒ•ã‚©ãƒ«ãƒ€ã®ID
        """
        try:
            # æ—¢å­˜ã®timelineãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢ï¼ˆå…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–å¯¾å¿œï¼‰
            query = f"name='timeline' and '{parent_folder_id}' in parents and mimeType='application/vnd.google-apps.folder' and trashed=false"
            results = service.files().list(
                q=query,
                supportsAllDrives=True,
                includeItemsFromAllDrives=True,
                fields='files(id, name)',
                pageSize=1
            ).execute()
            
            files = results.get('files', [])
            if files:
                return files[0]['id']
            
            # ãªã‘ã‚Œã°ä½œæˆï¼ˆå…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–å¯¾å¿œï¼‰
            folder_metadata = {
                'name': 'timeline',
                'mimeType': 'application/vnd.google-apps.folder',
                'parents': [parent_folder_id]
            }
            
            folder = service.files().create(
                body=folder_metadata,
                supportsAllDrives=True,
                fields='id'
            ).execute()
            
            print(f"ğŸ“ timelineãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸï¼ˆGoogle Driveï¼‰")
            return folder.get('id')
            
        except Exception as e:
            # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            error_msg = str(e)
            if "File not found" in error_msg or "404" in error_msg:
                print(f"âŒ è¦ªãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆID: {parent_folder_id}ï¼‰")
                print("   äº‹ä»¶ãƒ•ã‚©ãƒ«ãƒ€ã®IDãŒç„¡åŠ¹ã‹ã€å‰Šé™¤ã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            else:
                print(f"âŒ timelineãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None
    
    def _get_mime_type(self, file_name: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ MIMEã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        
        Args:
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
        
        Returns:
            MIMEã‚¿ã‚¤ãƒ—
        """
        extension = os.path.splitext(file_name)[1].lower()
        mime_types = {
            '.json': 'application/json',
            '.md': 'text/markdown',
            '.txt': 'text/plain',
            '.html': 'text/html',
            '.htm': 'text/html'
        }
        return mime_types.get(extension, 'application/octet-stream')


def main():
    """ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("="*80)
    print("æ™‚ç³»åˆ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼çµ„ã¿ç«‹ã¦ãƒ„ãƒ¼ãƒ« - ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ¢ãƒ¼ãƒ‰")
    print("="*80)
    print("\næ³¨æ„: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯é€šå¸¸ run_phase1_multi.py ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚")
    print("ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€case_manager ã¨ current_case ãŒå¿…è¦ã§ã™ã€‚")
    

if __name__ == "__main__":
    main()
